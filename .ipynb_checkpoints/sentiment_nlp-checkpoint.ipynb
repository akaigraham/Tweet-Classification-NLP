{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Classification (Module 4 Project - Kai Graham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Process - CRISP-DM\n",
    "I will be following the Cross-Industry Standard Process for Data Mining (CRISP-DM), with the following iterative steps.\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding\n",
    "I will be building a classifier to sort tweets based on sentiment (positive vs. negative vs. neutral).\n",
    "\n",
    "[...] Further information needed about stakeholders, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "The dataset used within this process comes from [...], obtained from [...]\n",
    "\n",
    "This section will focus on importing and exploring the data available to us as we begin to think about modeling and text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset and begin exploring\n",
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin_1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# more information about dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 9,093 total rows and three columns, one containing the tweet text, one containing a product / company sentiment is directed at, and the third is the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text             product  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "          sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns so they are easier to work with \n",
    "df.columns = ['text', 'product', 'sentiment']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text            1\n",
       "product      5802\n",
       "sentiment       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there only appears to be one missing value within the text column, there is a large number of missing values within the product column.  Start by handling the text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text product                           sentiment\n",
       "6  NaN     NaN  No emotion toward brand or product"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display missing entry\n",
    "df.loc[df['text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       9092 non-null   object\n",
      " 1   product    3291 non-null   object\n",
      " 2   sentiment  9092 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 284.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# drop as text is missing\n",
    "clean_df = df.dropna(subset=['text'])\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there appear to be quite a bit of missing product entries - examine further\n",
    "missing_products = df.loc[df['product'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5298\n",
       "Positive emotion                       306\n",
       "I can't tell                           147\n",
       "Negative emotion                        51\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment counts of missing product rows\n",
    "missing_products['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is missing just over 5,800 brand/product distinctions.  The majority of these are labeled as \"neutral sentiment\" which makes logical sense, but there are also a handful with different labels.  As our classifier is focused on classifying the sentiment of tweets and not as concerned with the products / brands within the text, we will drop the product column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9092\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       9092 non-null   object\n",
      " 1   sentiment  9092 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 213.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# drop product column\n",
    "clean_df = clean_df.drop(['product'], axis=1)\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         False\n",
       "sentiment    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for additional missing values\n",
    "clean_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicated entries\n",
    "clean_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicated values\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "clean_df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset no longer contains unnecessary columns, has handled missing values, and has removed duplicate entries.  Next, further explore our dataset prior to data preprocessing and preparation for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9070 entries, 0 to 9092\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       9070 non-null   object\n",
      " 1   sentiment  9070 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 212.6+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now contains 9,070 labeled tweets.  Explore breakdown of sentiment labels and start to think about if any class imbalance will need to be handled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5375\n",
       "Positive emotion                      2970\n",
       "Negative emotion                       569\n",
       "I can't tell                           156\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value counts of sentiment column\n",
    "clean_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks to @mention for publishing the news of @mention new medical Apps at the #sxswi conf. blog {link} #sxsw #sxswh'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\x89ÛÏ@mention &quot;Apple has opened a pop-up store in Austin so the nerds in town for #SXSW can get their new iPads. {link} #wow'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Just what America needs. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The queue at the Apple Store in Austin is FOUR blocks long. Crazy stuff! #sxsw'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Hope it's better than wave RT @mention Buzz is: Google's previewing a social networking platform at #SXSW: {link}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SYD #SXSW crew your iPhone extra juice pods have been procured.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Why Barry Diller thinks iPad only content is nuts @mention #SXSW {link}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# examing some of the tweets labeled as \"I can't tell\" -- print first 5\n",
    "for i in list(range(7)):\n",
    "    display(clean_df.loc[clean_df['sentiment'] == \"I can't tell\"].iloc[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After briefly reviewing some of the tweets that were labeled as \"I can't tell\", it seems there are some tweets that could fall into the netural category as well as some I see that could fall into the negative category (tweets regarding queue length as the stores).\n",
    "\n",
    "For this reason, I am choosing to drop all entries labeled as \"I can't tell\". Given that there are only ~156 of these entries, a large portion of the dataset is not being removed, and I feel comfortable with the amount of data remaining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8914 entries, 0 to 9092\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       8914 non-null   object\n",
      " 1   sentiment  8914 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 208.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# remove all I can't tell from the dataset as we don't have proper labels for these\n",
    "clean_df = clean_df.loc[clean_df['sentiment'] != \"I can't tell\"]\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5375\n",
       "Positive emotion                      2970\n",
       "Negative emotion                       569\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have handled unlabeled data (by removing), looking at the remaining value counts, we can see that the majority of tweets in our dataset are labeled as \"No emotion toward brand or product\", or in otherwords, neutral. Additionally, there are 2,970 positively labeled tweets, and only 569 negatively rated tweets.  \n",
    "\n",
    "Our dataset clearly has some class imbalanced.  This will be kept in mind and handled in the next section, data preparation. \n",
    "\n",
    "Prior to moving onto further data preparation, explore various corpus statistics, and vectorize our clean_df to get a sense of which words are most common, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce frequency distribution\n",
    "data = clean_df['text']\n",
    "labels = clean_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, FreqDist\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize entire text\n",
    "tokenized_tweets = list(map(nltk.word_tokenize, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13075"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total vocabulary of our dataset\n",
    "total_vocab = set()\n",
    "for tweet in tokenized_tweets:\n",
    "    total_vocab.update(tweet)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '@',\n",
       " 'wesley83',\n",
       " 'I',\n",
       " 'have',\n",
       " 'a',\n",
       " '3G',\n",
       " 'iPhone',\n",
       " '.',\n",
       " 'After',\n",
       " '3',\n",
       " 'hrs',\n",
       " 'tweeting',\n",
       " 'at',\n",
       " '#',\n",
       " 'RISE_Austin',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'dead']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine first 20 characters of first tokenized tweet\n",
    "tokenized_tweets[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 13,075 total unique words within our dataset, prior to removing any stopwords or punctuation.  Just from the one tweet above, we can see our dataset will benefit from removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 15593),\n",
       " ('@', 7075),\n",
       " ('mention', 7005),\n",
       " ('.', 5480),\n",
       " ('SXSW', 4696),\n",
       " ('sxsw', 4432),\n",
       " ('link', 4247),\n",
       " ('}', 4234),\n",
       " ('{', 4232),\n",
       " ('the', 3855),\n",
       " ('to', 3460),\n",
       " (',', 3459),\n",
       " ('RT', 2899),\n",
       " ('at', 2808),\n",
       " (';', 2748),\n",
       " ('&', 2657),\n",
       " ('for', 2399),\n",
       " ('!', 2370),\n",
       " ('a', 2128),\n",
       " ('iPad', 2088),\n",
       " ('Google', 2077),\n",
       " (':', 2030),\n",
       " ('Apple', 1850),\n",
       " ('in', 1804),\n",
       " ('quot', 1657),\n",
       " ('of', 1654),\n",
       " ('?', 1616),\n",
       " ('is', 1605),\n",
       " ('and', 1509),\n",
       " ('I', 1425),\n",
       " ('iPhone', 1286),\n",
       " ('on', 1241),\n",
       " (\"'s\", 1212),\n",
       " ('2', 1104),\n",
       " ('store', 1036),\n",
       " ('-', 957),\n",
       " ('you', 934),\n",
       " ('Austin', 890),\n",
       " ('an', 837),\n",
       " ('amp', 827),\n",
       " ('with', 794),\n",
       " (')', 788),\n",
       " ('it', 768),\n",
       " ('up', 766),\n",
       " ('(', 760),\n",
       " ('my', 694),\n",
       " ('app', 631),\n",
       " ('...', 577),\n",
       " ('Circles', 566),\n",
       " ('new', 560)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display frequency distribution of unprocessed dataset\n",
    "tweets_concat = []\n",
    "for tweet in tokenized_tweets:\n",
    "    tweets_concat += tweet\n",
    "    \n",
    "tweet_freqdist = FreqDist(tweets_concat)\n",
    "tweet_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top 50 most common words in our corpus, we can see that the majority of these are stopwords or punctuation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with bigrams and other feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'binary_clean_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-dec67916671e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# split dataset into train and test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_clean_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'binary_clean_df' is not defined"
     ]
    }
   ],
   "source": [
    "# split dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(binary_clean_df, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into data and target\n",
    "train_data = train_df['text']\n",
    "train_target = train_df['emotion']\n",
    "\n",
    "test_data = test_df['text']\n",
    "test_target = test_df['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in stop words from english language\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to process a single tweet\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Input: tweet of type str\n",
    "    Function tokenizes tweet using function from nltk\n",
    "    Lowercase every token, remove any stopwords found in stopwords_list from the tokenized article, \n",
    "    and return the results\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use map function to call process_tweet on our data\n",
    "processed_data = list(map(process_tweet, train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['google',\n",
       " 'crisis',\n",
       " 'response',\n",
       " 'site',\n",
       " 'w/',\n",
       " 'good',\n",
       " 'info',\n",
       " 'japanese',\n",
       " 'earthquake/tsunami',\n",
       " 'link',\n",
       " 'sxsw',\n",
       " 'sxswi']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7684    #Google Crisis Response has a site up w/ good ...\n",
       "9063    @mention You should get the iPad 2  to save yo...\n",
       "8457    It was either go to #SXSW or wait in line and ...\n",
       "2040    Sweet... Apple listened to us!  A temp Apple S...\n",
       "285     At #SXSW, Apple schools the marketing experts ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like our tokenizing worked properly, as well as the removal of some stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5376"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total vocabulary size of our training set\n",
    "total_vocab = set()\n",
    "for tweet in processed_data:\n",
    "    total_vocab.update(tweet)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of unique words in our training set is 5374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxsw', 2758),\n",
       " ('mention', 1878),\n",
       " ('link', 989),\n",
       " ('ipad', 891),\n",
       " ('rt', 810),\n",
       " ('apple', 769),\n",
       " ('google', 656),\n",
       " ('iphone', 514),\n",
       " ('quot', 479),\n",
       " ('store', 424),\n",
       " ('2', 418),\n",
       " (\"'s\", 412),\n",
       " ('app', 325),\n",
       " ('new', 295),\n",
       " ('austin', 241),\n",
       " ('android', 174),\n",
       " (\"n't\", 172),\n",
       " ('amp', 169),\n",
       " ('ipad2', 166),\n",
       " ('launch', 135),\n",
       " ('get', 134),\n",
       " ('pop-up', 121),\n",
       " ('one', 120),\n",
       " ('time', 116),\n",
       " ('social', 113),\n",
       " ('great', 112),\n",
       " ('circles', 111),\n",
       " ('party', 107),\n",
       " ('today', 101),\n",
       " ('line', 100),\n",
       " ('like', 100),\n",
       " ('free', 100),\n",
       " ('via', 97),\n",
       " (\"'m\", 97),\n",
       " ('cool', 96),\n",
       " ('apps', 89),\n",
       " ('people', 87),\n",
       " ('maps', 87),\n",
       " ('day', 87),\n",
       " ('go', 83),\n",
       " ('good', 79),\n",
       " ('sxswi', 79),\n",
       " ('got', 77),\n",
       " ('love', 75),\n",
       " ('mobile', 75),\n",
       " ('network', 72),\n",
       " ('awesome', 71),\n",
       " ('opening', 70),\n",
       " ('temporary', 68),\n",
       " (\"'re\", 67),\n",
       " ('w/', 66),\n",
       " ('see', 66),\n",
       " ('check', 65),\n",
       " ('downtown', 64),\n",
       " ('need', 64),\n",
       " ('\\x89ûï', 59),\n",
       " ('thanks', 58),\n",
       " ('first', 58),\n",
       " ('best', 58),\n",
       " ('called', 57),\n",
       " ('going', 56),\n",
       " ('popup', 55),\n",
       " ('design', 54),\n",
       " ('major', 54),\n",
       " ('come', 53),\n",
       " ('open', 51),\n",
       " ('users', 51),\n",
       " ('next', 51),\n",
       " ('mayer', 50),\n",
       " ('around', 50),\n",
       " ('use', 49),\n",
       " ('ever', 49),\n",
       " ('even', 49),\n",
       " (\"'ve\", 48),\n",
       " ('think', 47),\n",
       " (\"'ll\", 47),\n",
       " ('heard', 46),\n",
       " ('want', 46),\n",
       " ('news', 45),\n",
       " ('win', 45),\n",
       " ('really', 45),\n",
       " ('us', 44),\n",
       " ('would', 44),\n",
       " ('know', 43),\n",
       " ('fun', 43),\n",
       " ('using', 42),\n",
       " ('coming', 41),\n",
       " ('tech', 41),\n",
       " ('nice', 40),\n",
       " ('big', 40),\n",
       " ('marissa', 40),\n",
       " ('technology', 40),\n",
       " ('could', 40),\n",
       " ('right', 39),\n",
       " ('4', 38),\n",
       " ('pop', 38),\n",
       " ('comes', 38),\n",
       " ('still', 38),\n",
       " ('game', 38),\n",
       " ('case', 38),\n",
       " ('video', 38),\n",
       " ('panel', 38),\n",
       " ('team', 38),\n",
       " ('set', 37),\n",
       " ('gt', 36),\n",
       " ('shop', 36),\n",
       " ('talk', 36),\n",
       " ('better', 36),\n",
       " ('possibly', 36),\n",
       " ('phone', 36),\n",
       " ('search', 35),\n",
       " ('smart', 35),\n",
       " ('work', 34),\n",
       " ('wow', 34),\n",
       " ('much', 34),\n",
       " ('wait', 33),\n",
       " ('year', 33),\n",
       " ('wins', 33),\n",
       " ('way', 33),\n",
       " ('thing', 33),\n",
       " ('buy', 32),\n",
       " ('ipads', 32),\n",
       " ('everyone', 32),\n",
       " ('music', 32),\n",
       " ('marketing', 31),\n",
       " ('away', 31),\n",
       " ('yes', 31),\n",
       " ('week', 31),\n",
       " ('make', 31),\n",
       " ('--', 31),\n",
       " ('also', 30),\n",
       " ('..', 30),\n",
       " ('looking', 30),\n",
       " ('back', 29),\n",
       " ('fast', 29),\n",
       " ('already', 29),\n",
       " ('may', 29),\n",
       " ('web', 29),\n",
       " ('rumor', 29),\n",
       " ('every', 29),\n",
       " ('well', 29),\n",
       " ('live', 28),\n",
       " ('session', 28),\n",
       " ('getting', 28),\n",
       " ('says', 27),\n",
       " ('booth', 27),\n",
       " ('look', 27),\n",
       " ('tonight', 27),\n",
       " ('begins', 27),\n",
       " ('last', 27),\n",
       " ('temp', 26),\n",
       " ('conferences', 26),\n",
       " ('looks', 26),\n",
       " ('amazing', 26),\n",
       " ('download', 26),\n",
       " ('hey', 26),\n",
       " ('made', 26),\n",
       " ('giving', 25),\n",
       " ('guy', 25),\n",
       " ('tomorrow', 25),\n",
       " ('excited', 25),\n",
       " ('1', 25),\n",
       " ('tapworthy', 25),\n",
       " ('product', 25),\n",
       " ('sxsw\\x89û\\x9d', 25),\n",
       " ('interactive', 25),\n",
       " ('ready', 25),\n",
       " ('keep', 25),\n",
       " ('winning', 24),\n",
       " ('3', 24),\n",
       " ('long', 24),\n",
       " ('show', 24),\n",
       " ('ca', 24),\n",
       " ('info', 23),\n",
       " ('u', 23),\n",
       " ('tweet', 23),\n",
       " ('far', 23),\n",
       " ('years', 23),\n",
       " ('action', 23),\n",
       " ('tv', 23),\n",
       " ('available', 23),\n",
       " ('white', 22),\n",
       " ('2011', 22),\n",
       " ('itunes', 22),\n",
       " ('oh', 22),\n",
       " ('many', 22),\n",
       " ('pretty', 22),\n",
       " ('take', 22),\n",
       " ('company', 22),\n",
       " ('bing', 22),\n",
       " ('makes', 22),\n",
       " ('traffic', 22),\n",
       " ('future', 22),\n",
       " ('lt', 21),\n",
       " ('digital', 21),\n",
       " ('market', 21),\n",
       " ('world', 21),\n",
       " ('attendees', 21),\n",
       " ('twitter', 21),\n",
       " ('release', 21)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create frequency distribution to see which words appear the most\n",
    "tweets_concat = []\n",
    "for tweet in processed_data:\n",
    "    tweets_concat += tweet\n",
    "    \n",
    "tweet_freqdist = FreqDist(tweets_concat)\n",
    "tweet_freqdist.most_common(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this is a frequency distribution across both of our sentiments (positive and negative), it is likely that the words presented above are the least important, as they are shared among both classses.  Knowing this, we will try to focus on words that appear frequently in one class but not the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import proper libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# vectorize train and test data\n",
    "tf_idf_data_train = vectorizer.fit_transform(train_data)\n",
    "tf_idf_data_test = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2654, 5199)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at shape of our vectorized data\n",
    "tf_idf_data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectorized data contains 2,654 tweets, with 5,199 unique words in the vocabulary.  The vast majority of these columns for any given tweet will be zero, since every article contains a small subset of the total vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Tweets: 16.66164280331575\n",
      "Percentage of columns containing 0: 0.9967952216189044\n"
     ]
    }
   ],
   "source": [
    "# display number of non-zero columns in the vectors\n",
    "non_zero_cols = tf_idf_data_train.nnz / float(tf_idf_data_train.shape[0])\n",
    "print(f'Average Number of Non-Zero Elements in Vectorized Tweets: {non_zero_cols}')\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(tf_idf_data_train.shape[1]))\n",
    "print(f'Percentage of columns containing 0: {percent_sparse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the average tweet contains ~16 non-zero columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate initial models\n",
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit naive bayes model\n",
    "nb_classifier.fit(tf_idf_data_train, train_target)\n",
    "nb_train_preds = nb_classifier.predict(tf_idf_data_train)\n",
    "nb_test_preds = nb_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit random forest classifier\n",
    "rf_classifier.fit(tf_idf_data_train, train_target)\n",
    "rf_train_preds = rf_classifier.predict(tf_idf_data_train)\n",
    "rf_test_preds = rf_classifier.predict(tf_idf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "nb_train_score = accuracy_score(train_target, nb_train_preds)\n",
    "nb_test_score = accuracy_score(test_target, nb_test_preds)\n",
    "rf_train_score = accuracy_score(train_target, rf_train_preds)\n",
    "rf_test_score = accuracy_score(test_target, rf_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes\n",
      "Training Accuracy: 0.8497 \t\t Testing Accuracy: 0.8418\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Random Forest\n",
      "Training Accuracy: 1.0 \t\t Testing Accuracy: 0.8655\n"
     ]
    }
   ],
   "source": [
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))\n",
    "print(\"\")\n",
    "print('-'*70)\n",
    "print(\"\")\n",
    "print('Random Forest')\n",
    "print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score, rf_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# think about further lemmatizing, n-grams, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "This is a classification task, aimed at classifying tweets based on their sentiment.  As a result, we will iterate through a number of potential models / hyperparameters to arrive at the optimal model for our task.\n",
    "\n",
    "Following metrics will be generated to help evaluate models:\n",
    "* Accuracy: total number of correct predictions out of total observations\n",
    "* Recall: number of true positives out of actual total positives.\n",
    "* Precision: number of true positives out of predicted positives.\n",
    "* F1 Score: harmonic mean of precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
