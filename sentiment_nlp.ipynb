{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Sentiment Classification\n",
    "### Module 4 Project - Kai Graham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Process (CRISP-DM)\n",
    "I will be following the Cross-Industry Standard Process for Data Mining to build a classifier that will determine the sentiment of tweets.  The CRISP-DM process includes the following key steps:\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business Understanding\n",
    "* Who are the stakeholders?\n",
    "* What Business Problems will this solve?\n",
    "* What problems are inside scope of this problem?\n",
    "* What problems are outside scope?\n",
    "* What data sources are available to us?\n",
    "* Timeline of project / deadlines?\n",
    "* Do stakeholders from different parts of the company all agree?\n",
    "\n",
    "My overall goal is to create a classifier that will successfully classify a tweet based on its sentiment into one of the following classes: positive sentiment, negative sentiment, or neutral sentiment.  Given data is limited to tweets related to Apple / Google products, the biggest stakeholders for this project are likely Google and Apple themselves.  Product managers / other managers within the company could use a tool like this to track public sentiment surrounding various product launches / software updates.  While out of the specific scope of this project, combining with time series metrics, the Company could track increases or changes to sentiment based on tweet classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding\n",
    "* What data is available to us, where does it live?\n",
    "* What is our target?\n",
    "* What predictors are available?\n",
    "* What is distribution of data?\n",
    "* EDA to show most common words and other corpus statistics.  Will require some initial processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main dataset used throughout this data science process will be coming from CrowdFlower via the following url: `https://data.world/crowdflower/brands-and-product-emotions`. \n",
    "\n",
    "The following summary of the dataset is provided on CrowdFlower:\n",
    "\n",
    "*Contributors evaluated tweets about multiple brands and products. The crowd was asked if the tweet expressed positive, negative, or no emotion towards a brand and/or product. If some emotion was expressed they were also asked to say which brand or product was the target of that emotion.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV \n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failure to specify 'latin1' encoding results in errors\n",
    "# error_df = pd.read_csv('judge-1377884607_tweet_product_company.csv')\n",
    "# error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "raw_df = pd.read_csv('judge-1377884607_tweet_product_company.csv', encoding='latin_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first rows of dataset\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# show info of df\n",
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above outputs, we can see that there are a total of 9,093 entries in our dataset, with a total of three columns.  Raw text from tweets is held in the `tweet_text` column; sentiment is held in the `is_there_an_emotion_directed_at_a_brand_or_product`; and the item of emotion direction is held in the `emotion_in_tweet_is_directed_at` column.  \n",
    "\n",
    "From first glance, we can likely drop the `emotion_in_tweet_is_directed_at` column as we are more interested in whether sentiment in a given tweet is positive, neutral, or negative based on the text.  Main predictors we will use is processed features derived from the `tweet_text` column.\n",
    "\n",
    "Our target variable, which can also be though of as our class labels are held in the `is_there_an_emotion_directed_at_a_brand_or_product` column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iPad                               946\n",
       "Apple                              661\n",
       "iPad or iPhone App                 470\n",
       "Google                             430\n",
       "iPhone                             297\n",
       "Other Google product or service    293\n",
       "Android App                         81\n",
       "Android                             78\n",
       "Other Apple product or service      35\n",
       "Name: emotion_in_tweet_is_directed_at, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display value counts\n",
    "display(raw_df['emotion_in_tweet_is_directed_at'].value_counts())\n",
    "display(raw_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprising given the origin of our dataset, the products identfied are either Apple or Google products.  Looking at sentiment, the majority of entries seem to fall under a neutral sentiment ('No emotion toward brand or product'), with the next largest group being tagged as 'Positive emotion'.  There is some clear class imbalance present with only 570 entries belonging to the 'Negative emotion' class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       product_brand  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...              iPhone   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  iPad or iPhone App   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...                iPad   \n",
       "3  @sxsw I hope this year's festival isn't as cra...  iPad or iPhone App   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...              Google   \n",
       "\n",
       "          sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns so easier to work with\n",
    "df = raw_df.copy()\n",
    "df.columns = ['text', 'product_brand', 'sentiment']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                1\n",
       "product_brand    5802\n",
       "sentiment           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore potential missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is only one missing value in the text column, 0 in the sentiment column, and a large number (5802) in the product_brand column.  Given we are planning to work the majority of the time with the text and sentiment columns, this will not likely pose a large issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text product_brand                           sentiment\n",
       "6  NaN           NaN  No emotion toward brand or product"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display missing value in the text column\n",
    "df.loc[df['text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Holler Gram for iPad on the iTunes App Store -...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Attn: All  #SXSW frineds, @mention Register fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Anyone at  #sxsw want to sell their old iPad?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>@mention Yup, but I don't have a third app yet...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5802 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text product_brand  \\\n",
       "5     @teachntech00 New iPad Apps For #SpeechTherapy...           NaN   \n",
       "6                                                   NaN           NaN   \n",
       "16    Holler Gram for iPad on the iTunes App Store -...           NaN   \n",
       "32    Attn: All  #SXSW frineds, @mention Register fo...           NaN   \n",
       "33        Anyone at  #sxsw want to sell their old iPad?           NaN   \n",
       "...                                                 ...           ...   \n",
       "9087  @mention Yup, but I don't have a third app yet...           NaN   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...           NaN   \n",
       "9090  Google's Zeiger, a physician never reported po...           NaN   \n",
       "9091  Some Verizon iPhone customers complained their...           NaN   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...           NaN   \n",
       "\n",
       "                               sentiment  \n",
       "5     No emotion toward brand or product  \n",
       "6     No emotion toward brand or product  \n",
       "16    No emotion toward brand or product  \n",
       "32    No emotion toward brand or product  \n",
       "33    No emotion toward brand or product  \n",
       "...                                  ...  \n",
       "9087  No emotion toward brand or product  \n",
       "9089  No emotion toward brand or product  \n",
       "9090  No emotion toward brand or product  \n",
       "9091  No emotion toward brand or product  \n",
       "9092  No emotion toward brand or product  \n",
       "\n",
       "[5802 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display missing values in the product_brand column\n",
    "df.loc[df['product_brand'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5298\n",
       "Positive emotion                       306\n",
       "I can't tell                           147\n",
       "Negative emotion                        51\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display sentiment breakdowns of missing product_brand entries\n",
    "df.loc[df['product_brand'].isna()]['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the majority of missing product_brand values are also labeled as no emotion twoard brand or product, which makes sense as a lot of the neutral-labeled tweets may not be directed at a specific brand or product, and therefore would be missing a product_brand tagging.  Additionally, this column will not be used in our process of tweet classification. \n",
    "\n",
    "Drop unnecessary columns and handle missing value for additional EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9092 entries, 0 to 9092\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       9092 non-null   object\n",
      " 1   sentiment  9092 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 213.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# drop product_brand column\n",
    "clean_df = df.drop(['product_brand'], axis=1)\n",
    "\n",
    "# handle missing values\n",
    "clean_df = clean_df.dropna(subset=['text'])\n",
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thanks to @mention for publishing the news of @mention new medical Apps at the #sxswi conf. blog {link} #sxsw #sxswh'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\x89ÛÏ@mention &quot;Apple has opened a pop-up store in Austin so the nerds in town for #SXSW can get their new iPads. {link} #wow'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Just what America needs. RT @mention Google to Launch Major New Social Network Called Circles, Possibly Today {link} #sxsw'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The queue at the Apple Store in Austin is FOUR blocks long. Crazy stuff! #sxsw'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Hope it's better than wave RT @mention Buzz is: Google's previewing a social networking platform at #SXSW: {link}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'SYD #SXSW crew your iPhone extra juice pods have been procured.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Why Barry Diller thinks iPad only content is nuts @mention #SXSW {link}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Gave into extreme temptation at #SXSW and bought an iPad 2... #impulse'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Catch 22\\x89Û_ I mean iPad 2 at #SXSW : {link}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Forgot my iPhone for #sxsw. Android only. Knife to a gun fight'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# further examine tweets labeled as \"I can't tell\"\n",
    "for i in range(10):\n",
    "    display(clean_df.loc[clean_df['sentiment'] == \"I can't tell\"].iloc[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a sample of the tweets labeled as \"I can't tell\", there is no clear class label that each should belong to.  Given this, and the small number of tweets with this class distinction, they will be removed from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataset into tweets and class_labels for additional EDA\n",
    "tweets = clean_df['text']\n",
    "class_labels = clean_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13212\n"
     ]
    }
   ],
   "source": [
    "# tokenize tweets and print the total vocabulary size of our dataset\n",
    "tokenized = list(map(nltk.word_tokenize, tweets.dropna())) \n",
    "raw_tweet_vocab = set()\n",
    "for tweet in tokenized:\n",
    "    raw_tweet_vocab.update(tweet)\n",
    "print(len(raw_tweet_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the text within the tweets, there is a total vocabulary size of just over 13,200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.414980202375716"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print average tweet size\n",
    "mean_tweet_size = []\n",
    "for tweet in tokenized:\n",
    "    mean_tweet_size.append(len(tweet))\n",
    "np.mean(mean_tweet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average tweet size within the dataset is just over 24 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 15875),\n",
       " ('@', 7194),\n",
       " ('mention', 7123),\n",
       " ('.', 5601),\n",
       " ('SXSW', 4787),\n",
       " ('sxsw', 4523),\n",
       " ('link', 4311),\n",
       " ('}', 4298),\n",
       " ('{', 4296),\n",
       " ('the', 3928),\n",
       " (',', 3533),\n",
       " ('to', 3521),\n",
       " ('RT', 2947),\n",
       " ('at', 2859),\n",
       " (';', 2800),\n",
       " ('&', 2707),\n",
       " ('for', 2440),\n",
       " ('!', 2398),\n",
       " ('a', 2174),\n",
       " ('Google', 2136),\n",
       " ('iPad', 2129),\n",
       " (':', 2075),\n",
       " ('Apple', 1882),\n",
       " ('in', 1833),\n",
       " ('quot', 1696)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display frequency distribution of raw dataset\n",
    "tweets_concat = []\n",
    "for tweet in tokenized:\n",
    "    tweets_concat += tweet\n",
    "    \n",
    "# display the 15 most common words\n",
    "unprocessed_freq_dist = nltk.FreqDist(tweets_concat)\n",
    "unprocessed_freq_dist.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From first glance, we can see a number of the top appear words / tokens are stopwords or punctuation.  For additional EDA processed, we will try removing stopwords to see if additional information can be extracted from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_tweet_process(tweet, stopwords_list):\n",
    "    \"\"\"\n",
    "    Function to intially process a tweet to assist in EDA / data understanding. \n",
    "    Input: tweet of type string, stopwords_list of words to remove\n",
    "    Returns: tokenized tweet, converted to lowercase, with all stopwords removed\n",
    "    \"\"\"\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # remove stopwords and lowercase\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "    \n",
    "    # return processed tweet\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up initial stopwords list\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``']\n",
    "stopwords_list += ['mention', 'sxsw', 'link', 'rt', 'quot', 'google', 'apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataset based on class label\n",
    "neutral_tweets = clean_df.loc[clean_df['sentiment'] == 'No emotion toward brand or product']\n",
    "positive_tweets = clean_df.loc[clean_df['sentiment'] == 'Positive emotion']\n",
    "negative_tweets = clean_df.loc[clean_df['sentiment'] == 'Negative emotion']\n",
    "ambig_tweets = clean_df.loc[clean_df['sentiment'] == \"I can't tell\"]\n",
    "all_tweets = clean_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the datasets\n",
    "processed_neutral = neutral_tweets['text'].apply(lambda x: initial_tweet_process(x, stopwords_list))\n",
    "processed_positive = positive_tweets['text'].apply(lambda x: initial_tweet_process(x, stopwords_list))\n",
    "processed_negative = negative_tweets['text'].apply(lambda x: initial_tweet_process(x, stopwords_list))\n",
    "processed_ambig = ambig_tweets['text'].apply(lambda x: initial_tweet_process(x, stopwords_list))\n",
    "processed_all = all_tweets['text'].apply(lambda x: initial_tweet_process(x, stopwords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tweets(tweets):\n",
    "    \"\"\"\n",
    "    Function to concatenate a list of tweets into one piece of text.\n",
    "    Input: tweets (list of tweets)\n",
    "    Returns: concatenated tweet string\n",
    "    \"\"\"\n",
    "    tweets_concat = []\n",
    "    for tweet in tweets:\n",
    "        tweets_concat += tweet\n",
    "    return tweets_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the datasets\n",
    "concat_neutral = concat_tweets(list(processed_neutral))\n",
    "concat_positive = concat_tweets(list(processed_positive))\n",
    "concat_negative = concat_tweets(list(processed_negative))\n",
    "concat_ambig = concat_tweets(list(processed_ambig))\n",
    "concat_all = concat_tweets(list(processed_all))\n",
    "\n",
    "# produce frequency distributions for datasets\n",
    "freqdist_neutral = nltk.FreqDist(concat_neutral)\n",
    "freqdist_positive = nltk.FreqDist(concat_positive)\n",
    "freqdist_negative = nltk.FreqDist(concat_negative)\n",
    "freqdist_ambig = nltk.FreqDist(concat_ambig)\n",
    "freqdist_all = nltk.FreqDist(concat_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Neutral Words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ipad', 1212),\n",
       " ('store', 867),\n",
       " ('iphone', 815),\n",
       " ('new', 678),\n",
       " (\"'s\", 648),\n",
       " ('austin', 630),\n",
       " ('amp', 601),\n",
       " ('2', 550),\n",
       " ('circles', 490),\n",
       " ('social', 481),\n",
       " ('launch', 465),\n",
       " ('today', 441),\n",
       " ('app', 355),\n",
       " ('network', 355),\n",
       " ('android', 350)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display top neutral words\n",
    "print('Top Neutral Words')\n",
    "freqdist_neutral.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Positive Words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ipad', 1003),\n",
       " ('store', 545),\n",
       " ('iphone', 523),\n",
       " (\"'s\", 493),\n",
       " ('2', 490),\n",
       " ('app', 396),\n",
       " ('new', 360),\n",
       " ('austin', 294),\n",
       " ('amp', 211),\n",
       " ('ipad2', 209),\n",
       " ('android', 198),\n",
       " ('launch', 160),\n",
       " ('get', 157),\n",
       " (\"n't\", 152),\n",
       " ('pop-up', 151)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display top positive words\n",
    "print('Top Positive Words')\n",
    "freqdist_positive.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Negative Words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ipad', 188),\n",
       " ('iphone', 162),\n",
       " (\"n't\", 87),\n",
       " (\"'s\", 77),\n",
       " ('2', 64),\n",
       " ('app', 60),\n",
       " ('store', 46),\n",
       " ('new', 43),\n",
       " ('like', 39),\n",
       " ('circles', 34),\n",
       " ('social', 31),\n",
       " ('apps', 30),\n",
       " ('people', 29),\n",
       " ('design', 28),\n",
       " ('need', 25)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display negative words\n",
    "print('Top Negative Words')\n",
    "freqdist_negative.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Ambiguous Words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ipad', 43),\n",
       " ('iphone', 32),\n",
       " ('store', 22),\n",
       " (\"'s\", 19),\n",
       " ('2', 18),\n",
       " ('circles', 17),\n",
       " ('austin', 16),\n",
       " (\"n't\", 14),\n",
       " ('social', 12),\n",
       " ('like', 11),\n",
       " ('go', 10),\n",
       " ('new', 9),\n",
       " ('pop-up', 9),\n",
       " ('line', 9),\n",
       " ('today', 8)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Top Ambiguous Words')\n",
    "freqdist_ambig.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words from All Tweets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ipad', 2446),\n",
       " ('iphone', 1532),\n",
       " ('store', 1480),\n",
       " (\"'s\", 1237),\n",
       " ('2', 1122),\n",
       " ('new', 1090),\n",
       " ('austin', 964),\n",
       " ('amp', 836),\n",
       " ('app', 817),\n",
       " ('circles', 658),\n",
       " ('launch', 653),\n",
       " ('social', 648),\n",
       " ('today', 580),\n",
       " ('android', 577),\n",
       " (\"n't\", 482)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display top all \n",
    "print('Top Words from All Tweets')\n",
    "freqdist_all.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the top words, we see again that the following words appear frequently in all class labels, and are therefore not as helpful in classification. Update stopwords list and reprint frequency distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create additional stopwords\n",
    "additional_stopwords = ['ipad', 'iphone', 'android', 'store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_normalized_word_freq(freq_dist, n=15):\n",
    "    \"\"\"\n",
    "    Print a normalized frequency distribution from a given distribution. Returns top n results. \n",
    "    \"\"\"\n",
    "    total_word_count = sum(freq_dist.values())\n",
    "    top = freq_dist.most_common(n)\n",
    "    \n",
    "    print('Word\\t\\t\\tNormalized Frequency')\n",
    "    for word in top:\n",
    "        normalized_freq = word[1] / total_word_count\n",
    "        print('{} \\t\\t\\t {:.4}'.format(word[0], normalized_freq))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t\tNormalized Frequency\n",
      "ipad \t\t\t 0.02441\n",
      "store \t\t\t 0.01746\n",
      "iphone \t\t\t 0.01641\n",
      "new \t\t\t 0.01365\n",
      "'s \t\t\t 0.01305\n",
      "austin \t\t\t 0.01269\n",
      "amp \t\t\t 0.0121\n",
      "2 \t\t\t 0.01108\n",
      "circles \t\t\t 0.009868\n",
      "social \t\t\t 0.009687\n",
      "launch \t\t\t 0.009365\n",
      "today \t\t\t 0.008881\n",
      "app \t\t\t 0.007149\n",
      "network \t\t\t 0.007149\n",
      "android \t\t\t 0.007049\n"
     ]
    }
   ],
   "source": [
    "# neutral normalized frequency\n",
    "print_normalized_word_freq(freqdist_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t\tNormalized Frequency\n",
      "ipad \t\t\t 0.03489\n",
      "store \t\t\t 0.01896\n",
      "iphone \t\t\t 0.01819\n",
      "'s \t\t\t 0.01715\n",
      "2 \t\t\t 0.01705\n",
      "app \t\t\t 0.01378\n",
      "new \t\t\t 0.01252\n",
      "austin \t\t\t 0.01023\n",
      "amp \t\t\t 0.00734\n",
      "ipad2 \t\t\t 0.007271\n",
      "android \t\t\t 0.006888\n",
      "launch \t\t\t 0.005566\n",
      "get \t\t\t 0.005462\n",
      "n't \t\t\t 0.005288\n",
      "pop-up \t\t\t 0.005253\n"
     ]
    }
   ],
   "source": [
    "# positive normalized frequency\n",
    "print_normalized_word_freq(freqdist_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t\tNormalized Frequency\n",
      "ipad \t\t\t 0.03262\n",
      "iphone \t\t\t 0.02811\n",
      "n't \t\t\t 0.01509\n",
      "'s \t\t\t 0.01336\n",
      "2 \t\t\t 0.0111\n",
      "app \t\t\t 0.01041\n",
      "store \t\t\t 0.007981\n",
      "new \t\t\t 0.00746\n",
      "like \t\t\t 0.006766\n",
      "circles \t\t\t 0.005899\n",
      "social \t\t\t 0.005378\n",
      "apps \t\t\t 0.005205\n",
      "people \t\t\t 0.005031\n",
      "design \t\t\t 0.004858\n",
      "need \t\t\t 0.004337\n"
     ]
    }
   ],
   "source": [
    "# negative normalized frequency\n",
    "print_normalized_word_freq(freqdist_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t\tNormalized Frequency\n",
      "ipad \t\t\t 0.02937\n",
      "iphone \t\t\t 0.02186\n",
      "store \t\t\t 0.01503\n",
      "'s \t\t\t 0.01298\n",
      "2 \t\t\t 0.0123\n",
      "circles \t\t\t 0.01161\n",
      "austin \t\t\t 0.01093\n",
      "n't \t\t\t 0.009563\n",
      "social \t\t\t 0.008197\n",
      "like \t\t\t 0.007514\n",
      "go \t\t\t 0.006831\n",
      "new \t\t\t 0.006148\n",
      "pop-up \t\t\t 0.006148\n",
      "line \t\t\t 0.006148\n",
      "today \t\t\t 0.005464\n"
     ]
    }
   ],
   "source": [
    "# ambig normalized frequency\n",
    "print_normalized_word_freq(freqdist_ambig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bigrams(tweets_concat, n=15):\n",
    "    \"\"\"\n",
    "    Function takes concatenated tweets and prints most common bigrams\n",
    "    \"\"\"\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tweets_concat)\n",
    "    tweet_scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    display(tweet_scored[:n])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ipad', '2'), 0.00896201715873847),\n",
       " (('social', 'network'), 0.0070084988117775),\n",
       " (('new', 'social'), 0.006424457244129375),\n",
       " (('called', 'circles'), 0.005175816651226487),\n",
       " (('network', 'called'), 0.00507511982921819),\n",
       " (('major', 'new'), 0.004531356990373383),\n",
       " (('launch', 'major'), 0.004370242075160108),\n",
       " (('pop-up', 'store'), 0.0038264792363153018),\n",
       " (('possibly', 'today'), 0.003745921778708664),\n",
       " (('circles', 'possibly'), 0.0037257824143070045),\n",
       " (('temporary', 'store'), 0.002960486567043944),\n",
       " (('store', 'austin'), 0.002678535465420711),\n",
       " (('iphone', 'app'), 0.0025979780078140735),\n",
       " (('downtown', 'austin'), 0.0023563056349941596),\n",
       " (('marissa', 'mayer'), 0.002215330084182543)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# neutral bigrams\n",
    "print_bigrams(concat_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ipad', '2'), 0.014646025395720994),\n",
       " (('iphone', 'app'), 0.004731257610019134),\n",
       " (('pop-up', 'store'), 0.003965907114280745),\n",
       " (('social', 'network'), 0.002922247347364759),\n",
       " (('temporary', 'store'), 0.002783092711775961),\n",
       " (('new', 'social'), 0.0026787267350843625),\n",
       " (('downtown', 'austin'), 0.002504783440598365),\n",
       " (('store', 'downtown'), 0.002435206122803966),\n",
       " (('ipad', 'app'), 0.0024004174639067665),\n",
       " (('network', 'called'), 0.0019829535571403724),\n",
       " (('called', 'circles'), 0.0019481648982431728),\n",
       " (('marissa', 'mayer'), 0.0019481648982431728),\n",
       " (('new', 'ipad'), 0.0019481648982431728),\n",
       " (('launch', 'major'), 0.0018785875804487736),\n",
       " (('major', 'new'), 0.0018785875804487736)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# positive bigrams\n",
    "print_bigrams(concat_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ipad', '2'), 0.008674531575294934),\n",
       " (('iphone', 'app'), 0.003990284524635669),\n",
       " (('ipad', 'design'), 0.003296321998612075),\n",
       " (('design', 'headaches'), 0.0029493407356002777),\n",
       " (('new', 'social'), 0.002775850104094379),\n",
       " (('social', 'network'), 0.0024288688410825814),\n",
       " (('news', 'apps'), 0.002255378209576683),\n",
       " (('fascist', 'company'), 0.002081887578070784),\n",
       " (('ipad', 'news'), 0.002081887578070784),\n",
       " (('major', 'new'), 0.002081887578070784),\n",
       " (('ca', \"n't\"), 0.0019083969465648854),\n",
       " (('called', 'circles'), 0.0019083969465648854),\n",
       " (('network', 'called'), 0.0019083969465648854),\n",
       " (('company', 'america'), 0.0017349063150589867),\n",
       " (('iphone', 'battery'), 0.0017349063150589867)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# negative bigrams\n",
    "print_bigrams(concat_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ipad', '2'), 0.01092896174863388),\n",
       " (('social', 'network'), 0.0047814207650273225),\n",
       " (('called', 'circles'), 0.004098360655737705),\n",
       " (('network', 'called'), 0.004098360655737705),\n",
       " (('pop-up', 'store'), 0.004098360655737705),\n",
       " (('new', 'social'), 0.0034153005464480873),\n",
       " (('store', 'austin'), 0.0034153005464480873),\n",
       " (('circles', 'possibly'), 0.00273224043715847),\n",
       " (('iphone', 'battery'), 0.0020491803278688526),\n",
       " (('iphone', 'game'), 0.0020491803278688526),\n",
       " (('launch', 'new'), 0.0020491803278688526),\n",
       " ((\"'re\", 'going'), 0.001366120218579235),\n",
       " ((\"'s\", 'gon'), 0.001366120218579235),\n",
       " (('ai', 'profile'), 0.001366120218579235),\n",
       " (('android', 'party'), 0.001366120218579235)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ambiguous bigrams\n",
    "print_bigrams(concat_ambig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the frequency distributions, we see a number of the same results showing up commonly among the different class labels.  We will move on to show the PMI for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pmi(tweets_concat, freq_filter=10):\n",
    "    \"\"\"\n",
    "    Function that takes concatenated tweets and a freq_filter number. Displays PMI scores. \n",
    "    \"\"\"\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    tweet_pmi_finder = BigramCollocationFinder.from_words(tweets_concat)\n",
    "    tweet_pmi_finder.apply_freq_filter(freq_filter)\n",
    "    tweet_pmi_scored = tweet_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
    "    display(tweet_pmi_scored)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('speak.', 'mark'), 12.277694226941772),\n",
       " (('lonely', 'planet'), 12.140190703191836),\n",
       " (('speech', 'therapy'), 11.899182603688041),\n",
       " (('augmented', 'reality'), 11.792267399771529),\n",
       " (('mark', 'belinsky'), 11.792267399771529),\n",
       " (('therapy', 'communication'), 11.761679079938107),\n",
       " (('communication', 'showcased'), 11.692731726220615),\n",
       " (('dwnld', 'groundlink'), 11.692731726220615),\n",
       " (('barry', 'diller'), 11.599622321829134),\n",
       " (('league', 'extraordinary'), 11.59962232182913),\n",
       " (('south', 'southwest'), 11.27769422694177),\n",
       " (('mike', 'tyson'), 11.277694226941769),\n",
       " (('exhibit', 'hall'), 11.207304899050374),\n",
       " (('interrupt', 'regularly'), 11.207304899050373),\n",
       " (('regularly', 'scheduled'), 11.207304899050373),\n",
       " (('afford', 'attend'), 11.178158553390855),\n",
       " (('living', 'social-type'), 11.140190703191838),\n",
       " (('red', 'cross'), 11.076060365772122),\n",
       " (('consider', 'saving'), 11.046840226857128),\n",
       " (('schools', 'marketing'), 10.938556842022185),\n",
       " (('showcased', 'conference'), 10.818262608304472),\n",
       " (('marketing', 'experts'), 10.813025959938328),\n",
       " (('150', 'million'), 10.776500083913213),\n",
       " (('contextual', 'discovery'), 10.7712057842437),\n",
       " (('convention', 'center'), 10.741641326701561),\n",
       " (('social-type', 'rewards'), 10.741641326701561),\n",
       " (('heads', 'sets'), 10.706537525745643),\n",
       " (('groupon', 'living'), 10.699618111805853),\n",
       " (('geek', 'programming'), 10.599622321829134),\n",
       " (('scheduled', 'geek'), 10.529232993937736),\n",
       " (('gon', 'na'), 10.512159480578793),\n",
       " (('wan', 'na'), 10.512159480578793),\n",
       " (('able', 'afford'), 10.470339304884165),\n",
       " (('physical', 'worlds'), 10.470339304884165),\n",
       " (('classical', 'newtwitter'), 10.466355790965668),\n",
       " (('navigation', 'schemas'), 10.450758935914651),\n",
       " (('core', 'action'), 10.43130120608941),\n",
       " (('business', 'cards'), 10.412623807027877),\n",
       " (('edchat', 'musedchat'), 10.351694808385549),\n",
       " (('\\x89ã_', 'edchat'), 10.351694808385547),\n",
       " (('\\x89÷_', '\\x89ã_'), 10.280833934472339),\n",
       " (('includes', 'uberguide'), 10.107769225499458),\n",
       " (('03/12/11', 'infektd'), 10.07606036577212),\n",
       " (('03/14/11', 'infektd'), 10.07606036577212),\n",
       " (('infektd', 'necro'), 10.07606036577212),\n",
       " (('infektd', 'zomb'), 10.07606036577212),\n",
       " (('valid', '4:00-7:59:59p'), 10.045033470151497),\n",
       " (('valid', '12:00-3:59:59p'), 10.045033470151495),\n",
       " (('uberguide', 'sponsored'), 10.017571416527879),\n",
       " (('searches', 'local'), 10.003771504965242),\n",
       " (('infektd', 'zlf'), 9.982950961380638),\n",
       " (('sponsored', 'mashable'), 9.970265701749524),\n",
       " (('03/11/11', 'infektd'), 9.938556842022185),\n",
       " (('plenty', 'else'), 9.932349467623242),\n",
       " (('codes', 'valid'), 9.896170084237013),\n",
       " (('featured', 'artists'), 9.893353524885844),\n",
       " (('attend', 'buy'), 9.825182022244265),\n",
       " (('local', 'information'), 9.792267399771529),\n",
       " (('hall', '3'), 9.692731726220615),\n",
       " (('schemas', 'uxdes'), 9.692731726220615),\n",
       " (('giving', 'away'), 9.600524288123525),\n",
       " (('earthquake', 'relief'), 9.555228202470678),\n",
       " (('design', 'headaches'), 9.512159480578793),\n",
       " (('booth', '437'), 9.491097865050962),\n",
       " (('digital', 'magazines'), 9.419050076187311),\n",
       " (('newtwitter', '\\x89÷¼'), 9.419050076187311),\n",
       " (('website', 'ranking'), 9.41589152086179),\n",
       " (('think', 'speak.'), 9.24207031721105),\n",
       " (('programming', 'big'), 9.207304899050373),\n",
       " (('connect', 'digital'), 9.145056458363651),\n",
       " (('trade', 'show'), 9.116774038760667),\n",
       " (('download', '20+'), 9.103196495709634),\n",
       " (('booth', 'exhibit'), 9.005671037880722),\n",
       " (('shop', 'core'), 8.909764085509888),\n",
       " (('brain', 'search'), 8.86830329080407),\n",
       " (('blog', 'post'), 8.834750731093042),\n",
       " (('designing', 'boomers'), 8.802914643971038),\n",
       " (('japan', 'earthquake'), 8.800340700307212),\n",
       " (('around', 'block'), 8.787445016314686),\n",
       " (('best', 'thing'), 8.742636632046928),\n",
       " (('40', 'maps'), 8.62763669799873),\n",
       " (('getting', 'ready'), 8.607842828634102),\n",
       " ((\"'ve\", 'heard'), 8.595870186968025),\n",
       " (('apps', 'speech'), 8.590429897548413),\n",
       " (('tracks', 'music'), 8.570335094860889),\n",
       " (('wonder', 'many'), 8.529232993937736),\n",
       " (('maps', 'usage'), 8.494237572581532),\n",
       " (('saving', 'japan'), 8.452417396886903),\n",
       " (('sampler', 'available'), 8.444804212777028),\n",
       " (('money', 'japan'), 8.44034456458633),\n",
       " (('thing', \"'ve\"), 8.33283578113423),\n",
       " (('launching', 'products'), 8.313559679498777),\n",
       " (('marissa', 'mayer'), 8.306673293913537),\n",
       " (('else', 'join'), 8.266466971518517),\n",
       " (('last', 'night'), 8.265721585275696),\n",
       " (('shop', 'report'), 8.185289435526878),\n",
       " (('post', 'hootsuite'), 8.144295101524573),\n",
       " (('chance', 'win'), 8.113328750841358),\n",
       " (('japan', 'relief'), 8.01584673605842),\n",
       " (('6th', 'congress'), 7.965029053383375),\n",
       " (('looks', 'like'), 7.8333300563934145),\n",
       " (('join', 'us'), 7.8135861210950015),\n",
       " (('make', 'sure'), 7.805206455479029),\n",
       " (('wo', \"n't\"), 7.760418533732189),\n",
       " (('worlds', 'mobile'), 7.760418533732189),\n",
       " (('sets', 'temporary'), 7.754792694352867),\n",
       " (('musedchat', 'sxswi'), 7.75413227088476),\n",
       " (('sxswi', 'classical'), 7.754132270884758),\n",
       " ((\"'re\", 'plenty'), 7.737294921311145),\n",
       " (('ca', \"n't\"), 7.720890169545553),\n",
       " (('music', 'sampler'), 7.71872693475356),\n",
       " (('mayer', 'connect'), 7.673961922200775),\n",
       " (('industry', 'party'), 7.667857057581585),\n",
       " (('rumor', 'opening'), 7.521102418566052),\n",
       " (('set', 'open'), 7.504428195463449),\n",
       " (('20+', 'free'), 7.484145104409196),\n",
       " (('6th', 'street'), 7.455473955320244),\n",
       " (('free', 'tracks'), 7.416789836607434),\n",
       " (('party', 'maggie'), 7.4137557765177995),\n",
       " (('hootsuite', 'mobile'), 7.282371236927544),\n",
       " (('usage', 'mobile'), 7.2534585450123075),\n",
       " (('anyone', 'know'), 7.239559098055002),\n",
       " (('--', 'gt'), 7.235852582730674),\n",
       " (('popup', 'shop'), 7.2308040947792485),\n",
       " (('mobile', 'updates'), 7.229903817033408),\n",
       " (('big', 'news'), 7.197967034471036),\n",
       " ((\"n't\", 'forget'), 7.175456033011033),\n",
       " (('last', 'year'), 7.173637186284976),\n",
       " (('groundlink', 'app'), 7.1279471074370875),\n",
       " (('location', 'location'), 7.125691133496721),\n",
       " (('opening', 'temporary'), 7.0889101982481435),\n",
       " (('gt', 'gt'), 7.052406441634744),\n",
       " (('network', 'called'), 6.991484864887816),\n",
       " (('opening', 'temp'), 6.930737337562885),\n",
       " (('products', \"'re\"), 6.9141726833952255),\n",
       " (('today', 'consider'), 6.814987476271613),\n",
       " (('possibly', 'today'), 6.807251827492006),\n",
       " (('let', 'know'), 6.686305457061181),\n",
       " ((\"'re\", 'launching'), 6.675233953178896),\n",
       " (('launch', 'major'), 6.673538160987302),\n",
       " (('social', 'network'), 6.6609975195157745),\n",
       " (('circles', 'possibly'), 6.647471383455237),\n",
       " (('called', 'circles'), 6.554866765971255),\n",
       " (('open', 'popup'), 6.407329507358364),\n",
       " (('gsd', 'amp'), 6.368401141117948),\n",
       " (('long', 'line'), 6.3595124705683155),\n",
       " (('pop-up', 'shop'), 6.345446660720372),\n",
       " (('social', 'media'), 6.321997453558604),\n",
       " (('relief', \"n't\"), 6.268565437402515),\n",
       " (('major', 'new'), 6.18171356261891),\n",
       " (('new', 'ubersocial'), 6.162059381000413),\n",
       " (('today', 'sxsw\\x89û\\x9d'), 6.143610223732981),\n",
       " (('downtown', 'austin'), 6.083316181774919),\n",
       " (('launch', 'groupon'), 6.075570403111309),\n",
       " (('q', 'amp'), 6.063546559589527),\n",
       " (('vs', 'android'), 6.01090768624687),\n",
       " (('amp', 'physical'), 5.994005626336449),\n",
       " (('ubersocial', 'iphone'), 5.863377731083116),\n",
       " (('austin', 'tx'), 5.840982684804558),\n",
       " (('store', 'includes'), 5.839734138607298),\n",
       " (('interfaces', 'new'), 5.838337048467514),\n",
       " (('new', 'navigation'), 5.807457735583542),\n",
       " (('free', 'sampler'), 5.785841130612095),\n",
       " (('amp', '80s'), 5.783438640396792),\n",
       " (('opening', 'pop-up'), 5.711197630700736),\n",
       " (('2', 'money'), 5.676906759058932),\n",
       " (('temporary', 'store'), 5.647089060664904),\n",
       " (('2', 'giveaway'), 5.6218653955009685),\n",
       " (('team', 'android'), 5.621164207131937),\n",
       " (('new', 'social'), 5.6020003886876175),\n",
       " (('updates', 'iphone'), 5.59776016423532),\n",
       " (('open', 'temporary'), 5.589920920636839),\n",
       " (('temp', 'store'), 5.537171368586865),\n",
       " (('h4ckers', 'amp'), 5.520404234562998),\n",
       " (('pop', 'store'), 5.505996741430302),\n",
       " (('pop-up', 'store'), 5.461222515353569),\n",
       " ((\"'s\", 'rumor'), 5.452417396886904),\n",
       " (('open', 'pop-up'), 5.432420488321199),\n",
       " (('store', 'downtown'), 5.424696639328452),\n",
       " (('austin', 'guide'), 5.399949976992767),\n",
       " (('ipad', '2s'), 5.3564483383561825),\n",
       " (('ipad', 'interfaces'), 5.3564483383561825),\n",
       " (('austin', 'convention'), 5.3493239039228),\n",
       " (('relief', 'via'), 5.347548279032949),\n",
       " (('come', 'see'), 5.345774836841729),\n",
       " ((\"'m\", 'going'), 5.337889008860516),\n",
       " (('iphone', 'charger'), 5.29669785721118),\n",
       " (('ipad', '2.'), 5.256912664805267),\n",
       " (('amp', 'bing'), 5.214072994726653),\n",
       " (('popup', 'store'), 5.209683748357605),\n",
       " (('see', 'us'), 5.190655770174825),\n",
       " (('opening', 'pop'), 5.169169770163602),\n",
       " ((\"n't\", 'need'), 5.137981327592371),\n",
       " (('digital', 'amp'), 5.123288643281414),\n",
       " (('free', 'music'), 5.1178228901633815),\n",
       " (('ipad', 'dj'), 5.093413932522388),\n",
       " (('mom', 'ipad'), 5.0808138957427555),\n",
       " (('austin', 'texas'), 5.078021882105405),\n",
       " (('amp', 'congress'), 5.056457134803209),\n",
       " (('ipad', '2'), 5.050822055797919),\n",
       " (('iphone', 'blackberry'), 4.9503397235032605),\n",
       " (('blackberry', 'amp'), 4.946168140434899),\n",
       " (('line', 'pop'), 4.923413355761642),\n",
       " (('new', 'blog'), 4.921462364286372),\n",
       " (('\\x89ûï', 'launch'), 4.878713073881999),\n",
       " (('gave', 'ipad'), 4.825933621657404),\n",
       " (('maps', 'mobile'), 4.788432909901786),\n",
       " (('iphone', 'battery'), 4.771424795724213),\n",
       " (('download', 'free'), 4.732905379260874),\n",
       " (('store', 'sold'), 4.702230614857363),\n",
       " (('via', 'twitter'), 4.669476373920311),\n",
       " (('austin', 'details'), 4.637449290719424),\n",
       " (('people', 'line'), 4.626796349403467),\n",
       " (('mobile', 'marissa'), 4.6019891711277054),\n",
       " (('win', 'free'), 4.590193564508176),\n",
       " (('6th', 'amp'), 4.5818047792271415),\n",
       " (('iphone', 'case'), 4.550454449456964),\n",
       " (('designing', 'ipad'), 4.512567540273466),\n",
       " (('iphone', 'app'), 4.468518113741904),\n",
       " (('guide', 'iphone'), 4.443539245540451),\n",
       " (('line', 'popup'), 4.4246074987902),\n",
       " (('ipad', 'design'), 4.356448338356184),\n",
       " (('buy', 'ipad'), 4.3564483383561825),\n",
       " (('tweet', 'new'), 4.329410438778897),\n",
       " (('store', '6th'), 4.254771637886142),\n",
       " ((\"'s\", 'marissa'), 4.248184344669294),\n",
       " (('iphone', 'game'), 4.232972259600793),\n",
       " (('new', 'ipads'), 4.224854507736307),\n",
       " (('news', 'circles'), 4.123825571718532),\n",
       " ((\"n't\", 'know'), 4.116562343957465),\n",
       " (('let', \"'s\"), 4.089847317502196),\n",
       " (('look', \"'s\"), 4.089847317502196),\n",
       " (('new', 'post'), 4.024555857250478),\n",
       " (('2', 'launch'), 4.010287038768642),\n",
       " (('win', 'ipad'), 3.954349894784837),\n",
       " (('laptop', 'ipad'), 3.8710215111859405),\n",
       " (('iphone', '4'), 3.8465039125187204),\n",
       " (('today', 'via'), 3.820301276168081),\n",
       " ((\"n't\", 'get'), 3.6800451172681683),\n",
       " (('year', \"'s\"), 3.653114747124034),\n",
       " (('new', 'think'), 3.6442837761323084),\n",
       " (('today', 'w/'), 3.5977567600509435),\n",
       " (('today', 'launch'), 3.5977567600509417),\n",
       " (('store', 'austin'), 3.595808555721206),\n",
       " (('twitter', 'amp'), 3.5204042345629976),\n",
       " (('bing', \"'s\"), 3.4731759570537015),\n",
       " (('away', 'ipad'), 3.458327952375395),\n",
       " (('ipad', 'apps'), 3.4462450087068284),\n",
       " (('launching', 'new'), 3.431980172419445),\n",
       " (('network', 'circles'), 3.4007625340634675),\n",
       " (('android', 'app'), 3.377175713745853),\n",
       " (('android', 'party'), 3.3548620874642303),\n",
       " (('ipad', '1'), 3.332601596401812),\n",
       " (('shop', 'austin'), 3.2841124911127544),\n",
       " (('ipad2', 'launch'), 3.1962773660668233),\n",
       " (('via', 'launch'), 3.179948330536881),\n",
       " (('amp', 'android'), 3.12664339491457),\n",
       " (('app', 'store'), 3.1229464263787214),\n",
       " (('ipad2', 'line'), 3.0973623399941346),\n",
       " (('ipad', 'designing'), 3.034520243468819),\n",
       " (('austin', 'ipad2'), 3.0211906595087292),\n",
       " (('store', 'congress'), 2.9917372320523477),\n",
       " (('iphone', 'apps'), 2.9681366699913703),\n",
       " (('launch', 'opening'), 2.831644820225222),\n",
       " (('today', 'sxswi'), 2.776852347384839),\n",
       " (('store', 'w/'), 2.7600069461365635),\n",
       " (('mobile', 'app'), 2.748174937977442),\n",
       " (('circles', 'today'), 2.7363305323966145),\n",
       " (('launch', 'new'), 2.7257113754761555),\n",
       " (('mayer', \"'s\"), 2.610679480803636),\n",
       " (('need', 'ipad'), 2.573546460023117),\n",
       " (('first', 'ipad'), 2.5490934162985788),\n",
       " ((\"'s\", 'pop-up'), 2.4813300888021423),\n",
       " (('2', 'pop-up'), 2.3553222038900365),\n",
       " (('iphone', 'android'), 2.2851098829359664),\n",
       " (('line', 'store'), 2.2016603014265783),\n",
       " (('2', 'line'), 2.121295082070187),\n",
       " (('store', 'ipad2'), 2.0630108352033574),\n",
       " (('austin', 'via'), 2.025620184145584),\n",
       " (('austin', 'ipad'), 1.9879776575317898),\n",
       " (('launch', 'via'), 1.9783144693672305),\n",
       " (('network', 'today'), 1.9282747626007222),\n",
       " (('line', 'ipad'), 1.920008362345115),\n",
       " (('ipad', 'app'), 1.8847731239641377),\n",
       " ((\"'s\", 'party'), 1.8812606956907807),\n",
       " (('launch', 'social'), 1.8286423320636978),\n",
       " (('2', \"'s\"), 1.8003407003072098),\n",
       " (('circles', 'social'), 1.7530912990565213),\n",
       " (('circles', 'launch'), 1.7087880724396847),\n",
       " ((\"'s\", 'free'), 1.69683612455335),\n",
       " (('store', 'line'), 1.6870871285968185),\n",
       " (('app', 'iphone'), 1.6272158597609625),\n",
       " (('app', 'amp'), 1.597165644866994),\n",
       " (('android', 'iphone'), 1.4777549608783644),\n",
       " (('free', 'ipad'), 1.4215433665780672),\n",
       " (('get', 'ipad'), 1.3915521393120969),\n",
       " (('new', 'ipad'), 1.3775716299219347),\n",
       " (('launch', \"'s\"), 1.3055760085576331),\n",
       " (('amp', 'iphone'), 1.2213068480565212),\n",
       " (('austin', 'store'), 1.1843823099947421),\n",
       " (('today', \"'s\"), 1.175577191528081),\n",
       " (('new', 'iphone'), 1.1087871102955056),\n",
       " (('new', 'android'), 1.0651978417478247),\n",
       " (('austin', 'amp'), 0.9760837183391864),\n",
       " (('amp', 'ipad'), 0.8256668757860872),\n",
       " (('ipad', 'today'), 0.8197410062422463),\n",
       " (('amp', \"'s\"), 0.7289908563744163),\n",
       " (('amp', 'amp'), 0.7221424611279161),\n",
       " ((\"'s\", 'new'), 0.555070573949255),\n",
       " ((\"'s\", 'amp'), 0.48798275687062187),\n",
       " (('iphone', 'ipad'), 0.38623180737883445),\n",
       " (('store', 'today'), 0.3770273879371402),\n",
       " (('ipad', 'store'), -0.15551233142206655),\n",
       " (('store', 'ipad'), -0.2335148434233396),\n",
       " (('ipad', 'iphone'), -0.7292454100411021)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# neutral pmi\n",
    "display_pmi(concat_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('speak.', 'mark'), 11.226060909579708),\n",
       " (('belinsky', '911tweets'), 11.10053002749585),\n",
       " (('mark', 'belinsky'), 11.10053002749585),\n",
       " (('holler', 'gram'), 10.811023410300866),\n",
       " (('physical', 'worlds'), 10.797217610775835),\n",
       " (('gon', 'na'), 10.723560569050527),\n",
       " (('convention', 'center'), 10.530915491108132),\n",
       " (('choice', 'awards'), 10.489095315413502),\n",
       " (('includes', 'uberguide'), 10.351591791663568),\n",
       " (('connect', 'digital'), 10.096777892634744),\n",
       " (('song', 'info'), 9.904132814692346),\n",
       " (('uberguide', 'sponsored'), 9.878137606159402),\n",
       " (('core', 'action'), 9.778824587912718),\n",
       " (('looking', 'forward'), 9.553635567608215),\n",
       " (('brain', 'search'), 9.37806400302476),\n",
       " (('911tweets', 'panel'), 9.327940523598922),\n",
       " (('marketing', 'experts'), 9.293175105438246),\n",
       " (('schools', 'marketing'), 9.22606090957971),\n",
       " (('think', 'speak.'), 9.196313566185657),\n",
       " (('giving', 'away'), 9.158946713721171),\n",
       " (('shop', 'core'), 9.118311054184383),\n",
       " (('best', 'andoid'), 8.856827099913989),\n",
       " (('marissa', 'mayer'), 8.716003217872446),\n",
       " (('video', 'streaming'), 8.71509899030233),\n",
       " (('route', 'around'), 8.634434678577543),\n",
       " (('ever', 'heard'), 8.58560329626685),\n",
       " (('takes', 'video'), 8.582204719804984),\n",
       " (('even', 'begins'), 8.526975199084594),\n",
       " (('begins', 'wins'), 8.480921383967804),\n",
       " (('awards', 'thanks'), 8.380190751917052),\n",
       " (('yes', 'gowalla'), 8.363564433329644),\n",
       " (('got', 'ta'), 8.287461454243852),\n",
       " (('go', 'conferences'), 8.21576592885183),\n",
       " (('network', 'called'), 8.184481805828312),\n",
       " (('around', 'traffic'), 8.063277977381418),\n",
       " (('around', 'block'), 8.00883019335904),\n",
       " (('possibly', 'today'), 7.99084444788568),\n",
       " (('worlds', 'mobile'), 7.930419506212624),\n",
       " (('circles', 'possibly'), 7.905034780986739),\n",
       " (('set', 'open'), 7.806334764507238),\n",
       " (('gowalla', 'wins'), 7.802849478855165),\n",
       " (('social', 'network'), 7.7897129040554525),\n",
       " (('called', 'circles'), 7.7480136127750665),\n",
       " (('7', 'people'), 7.705638661053264),\n",
       " (('rumor', 'opening'), 7.699515095083875),\n",
       " (('gt', 'gt'), 7.680881901785856),\n",
       " (('ca', \"n't\"), 7.514186296376334),\n",
       " (('cool', 'technology'), 7.406633155221531),\n",
       " (('launch', 'major'), 7.361339768215132),\n",
       " (('comes', 'cool'), 7.303740273401587),\n",
       " (('opening', 'temporary'), 7.3021231153992705),\n",
       " (('live', 'video'), 7.255667371665032),\n",
       " (('40', 'maps'), 7.248780986079792),\n",
       " (('popup', 'shop'), 7.1963135661856565),\n",
       " (('love', 'comes'), 7.054800697453578),\n",
       " (('technology', 'one'), 7.036236350699692),\n",
       " (('every', 'day'), 7.005970539219801),\n",
       " (('heard', \"n't\"), 6.932329706523001),\n",
       " (('looks', 'like'), 6.87859772258521),\n",
       " (('people', 'work'), 6.793101502303603),\n",
       " (('amp', 'physical'), 6.76799612670632),\n",
       " (('open', 'popup'), 6.666931707841821),\n",
       " (('opening', 'temp'), 6.629125767192477),\n",
       " (('android', 'choice'), 6.5601784134749845),\n",
       " (('wins', 'best'), 6.559146551273308),\n",
       " (('pop-up', 'shop'), 6.48147078291759),\n",
       " (('downtown', 'austin'), 6.321844448269516),\n",
       " (('new', 'ubersocial'), 6.31917031397119),\n",
       " (('digital', 'amp'), 6.282569299536078),\n",
       " (('android', 'market'), 6.248780986079792),\n",
       " (('first', 'day'), 6.240274768799409),\n",
       " ((\"n't\", 'go'), 6.225226258100896),\n",
       " (('major', 'new'), 6.191414766772819),\n",
       " (('andoid', 'app'), 6.181666790221255),\n",
       " (('team', 'android'), 6.149245312528878),\n",
       " ((\"n't\", 'wait'), 6.122523305471301),\n",
       " (('austin', 'convention'), 6.108850724935319),\n",
       " (('app', 'song'), 5.918632384387461),\n",
       " (('ubersocial', 'iphone'), 5.709966946162526),\n",
       " (('open', 'pop-up'), 5.676454481960327),\n",
       " (('2', 'years'), 5.662881366104584),\n",
       " (('new', 'social'), 5.631760544279217),\n",
       " (('temp', 'store'), 5.630713181665),\n",
       " (('store', 'includes'), 5.583407466886641),\n",
       " (('temporary', 'store'), 5.56710565455754),\n",
       " (('2', 'takes'), 5.552457376410933),\n",
       " (('maps', 'mobile'), 5.526029251133288),\n",
       " (('store', 'downtown'), 5.390762388944246),\n",
       " (('tweet', 'new'), 5.380570858635336),\n",
       " (('opening', 'pop-up'), 5.361114565782074),\n",
       " (('pop', 'store'), 5.33012103760441),\n",
       " (('pop-up', 'store'), 5.315396265476242),\n",
       " (('iphone', 'charger'), 5.294929446883682),\n",
       " ((\"'s\", 'rumor'), 5.280617073201798),\n",
       " ((\"'s\", 'ever'), 5.225121960610094),\n",
       " (('popup', 'store'), 5.2009039317561925),\n",
       " (('network', 'circles'), 4.940658690717461),\n",
       " (('ipad', '2s'), 4.725440302268751),\n",
       " (('ipad', 'envy'), 4.7034139959387495),\n",
       " (('ipad', '2'), 4.621956003752512),\n",
       " (('new', 'think'), 4.2894229705771405),\n",
       " (('iphone', '4'), 4.282856614583107),\n",
       " (('2', 'launch'), 4.280377830974132),\n",
       " ((\"'s\", 'marissa'), 4.255526092238968),\n",
       " (('iphone', 'app'), 4.238462495224653),\n",
       " (('app', 'team'), 4.149245312528878),\n",
       " (('iphone', 'case'), 4.095858099781855),\n",
       " (('ipad', 'design'), 4.092456286684648),\n",
       " (('users', '2'), 3.9538199387927015),\n",
       " (('launch', 'via'), 3.904132814692346),\n",
       " (('one', \"'s\"), 3.8456800164852503),\n",
       " (('iphone', 'game'), 3.716225936634208),\n",
       " (('win', 'ipad'), 3.677418787405804),\n",
       " (('iphone', 'apps'), 3.6532443561505854),\n",
       " (('hollergram', 'ipad'), 3.639283658519034),\n",
       " (('away', 'ipad'), 3.3814859010513896),\n",
       " (('ipad2', 'launch'), 3.241167801969917),\n",
       " (('store', 'austin'), 3.2216783639413045),\n",
       " (('android', 'app'), 3.0758721261986572),\n",
       " (('ipad', 'case'), 2.934026924080168),\n",
       " (('austin', 'amp'), 2.697606798814922),\n",
       " (('store', 'ipad2'), 2.537113814612706),\n",
       " (('line', 'store'), 2.4948429111567307),\n",
       " (('got', 'ipad'), 2.404818404882011),\n",
       " (('need', 'ipad'), 2.393458542717463),\n",
       " (('ipad', 'app'), 2.3200853563872457),\n",
       " (('launch', 'new'), 2.31917031397119),\n",
       " ((\"'s\", 'pop-up'), 2.2121373353190315),\n",
       " (('new', 'iphone'), 2.1953937733327678),\n",
       " (('ipad', 'apps'), 2.1663178064828514),\n",
       " (('new', 'ipad'), 2.156419345416616),\n",
       " (('new', 'android'), 2.149245312528876),\n",
       " (('app', 'store'), 2.135948489915421),\n",
       " (('2', \"'s\"), 1.7362965569779902),\n",
       " (('iphone', 'android'), 1.7359621546954713),\n",
       " (('austin', 'ipad'), 1.548135770460842),\n",
       " (('get', 'ipad'), 1.5462967707970598),\n",
       " (('amp', 'ipad'), 1.5121357537602584),\n",
       " (('new', 'app'), 1.0117417887789415),\n",
       " (('austin', 'store'), 0.98067026443751),\n",
       " (('app', 'iphone'), 0.7359621546954713),\n",
       " ((\"'s\", 'ipad'), 0.2174017781981341),\n",
       " (('store', 'ipad'), 0.07273319491175911),\n",
       " (('ipad', \"'s\"), -0.19763572108070804),\n",
       " (('iphone', 'ipad'), -0.38239469450065044),\n",
       " (('app', 'ipad'), -0.4665110055035626)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# positive pmi\n",
    "display_pmi(concat_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('fascist', 'company'), 8.463107276780697),\n",
       " (('company', 'america'), 8.422465292283349),\n",
       " (('network', 'called'), 8.344955925062434),\n",
       " (('design', 'headaches'), 7.685499698117143),\n",
       " (('launch', 'major'), 7.547996174367208),\n",
       " (('social', 'network'), 7.439122636236958),\n",
       " (('called', 'circles'), 7.164383679420613),\n",
       " (('news', 'apps'), 7.116478741265009),\n",
       " (('major', 'new'), 6.844197444136203),\n",
       " (('new', 'social'), 6.112393555085774),\n",
       " (('ca', \"n't\"), 6.0499111243260195),\n",
       " (('ipad', '2'), 4.582121958271836),\n",
       " (('iphone', 'battery'), 4.568042116568966),\n",
       " (('ipad', 'design'), 4.3788383598830904),\n",
       " (('ipad', 'news'), 4.353303267775955),\n",
       " (('iphone', 'app'), 3.769675977738615)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# negative pmi\n",
    "display_pmi(concat_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ipad', '2'), 4.919510082139633)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_pmi(concat_ambig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at PMI scores, we can see some trends starting to emerge within our dataset.  Looking at positive tweets, we see combinations such as ('choice', 'awards'), ('league', 'extraordinary'); which contrast some of the combinations showing in the negative labeled tweets such as: ('fascist', 'company'), ('design', 'headaches'), etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "Leverage information learned during data understanding phase to preprocess dataset and prepare data for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column                                              Non-Null Count  Dtype \n",
      "---  ------                                              --------------  ----- \n",
      " 0   tweet_text                                          9092 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at                     3291 non-null   object\n",
      " 2   is_there_an_emotion_directed_at_a_brand_or_product  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in copy of dataset\n",
    "clean_df = raw_df.copy()\n",
    "\n",
    "# relabel columns\n",
    "clean_df.columns = ['text', 'product_brand', 'sentiment']\n",
    "\n",
    "# drop product_brand column, handle missing values and duplicates\n",
    "clean_df = clean_df.drop('product_brand', axis=1)\n",
    "clean_df = clean_df.dropna()\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "\n",
    "# remove ambiguous tweets\n",
    "clean_df = clean_df.loc[clean_df['sentiment'] != \"I can't tell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataset into text and class_labels\n",
    "text = clean_df['text']\n",
    "class_labels = clean_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tweets and labels into train and test sets for validation purposes\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, class_labels, stratify=class_labels, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update stopwords list per our data understanding findings\n",
    "updated_stopwords = stopwords_list + additional_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet, stopwords_list):\n",
    "    \"\"\"\n",
    "    Function to preprocess a tweet. \n",
    "    Takes: tweet, stopwords list\n",
    "    Returns: processed tweet with stopwords removed and converted to lowercase\n",
    "    \"\"\"\n",
    "    processed = re.sub(\"\\'\", '', tweet) # handle apostrophes\n",
    "    processed = re.sub('\\s+', ' ', processed) # handle excess white space\n",
    "    tokens = nltk.word_tokenize(processed)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "    return ' '.join(stopwords_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess train and test sets\n",
    "X_train_preprocessed = X_train.apply(lambda x: preprocess_tweet(x, updated_stopwords))\n",
    "X_test_preprocessed = X_test.apply(lambda x: preprocess_tweet(x, updated_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data into train and test sets, as well as, preprocessed both train and test sets, we are ready to vectorize our data.  We have chosen to use TF-IDF vectorization for its benefits in classification and finding words that are unique per class label. Try a number of vectorizers to see if there is one that performs better over the other (count vectorized vs. TF-IDF vectorized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorizers with unigram and bigrams\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2), analyzer='word', min_df=5)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), analyzer='word', min_df=5)\n",
    "\n",
    "# fit to preprocessed data\n",
    "X_train_count = count_vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_count = count_vectorizer.transform(X_test_preprocessed) \n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_preprocessed) \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_preprocessed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8951 unique tokens in the processed training set.\n"
     ]
    }
   ],
   "source": [
    "# tokenize datasets\n",
    "tokenized_X_train = X_train_preprocessed.map(nltk.word_tokenize).values\n",
    "tokenized_X_test = X_test_preprocessed.map(nltk.word_tokenize).values\n",
    "\n",
    "# get total training vocabulary size\n",
    "total_train_vocab = set(word for tweet in tokenized_X_train for word in tweet)\n",
    "train_vocab_size = len(total_train_vocab)\n",
    "print(f'There are {train_vocab_size} unique tokens in the processed training set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_vectors(vocab):\n",
    "    \"\"\"\n",
    "    Returns appropriate vectors from GloVe file.\n",
    "    Input: vocabulary set to use.\n",
    "    \"\"\"\n",
    "    glove = {}\n",
    "    with open('glove.6B.50d.txt', 'rb') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode('utf-8')\n",
    "            if word in vocab:\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                glove[word] = vector\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = glove_vectors(total_train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate vectorizer objects with glove\n",
    "w2v_vectorizer = W2vVectorizer(glove)\n",
    "\n",
    "# transform training and testing data\n",
    "X_train_w2v = w2v_vectorizer.transform(tokenized_X_train)\n",
    "X_test_w2v = w2v_vectorizer.transform(tokenized_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have vectorized our datasets, we are ready to move on to the modeling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification task, tasked with classifying the sentiment of tweets based on the text within the tweet. Three primary models will be relied on for classification:\n",
    "1. Random Forests\n",
    "2. Linear SVM\n",
    "3. Neural Networks\n",
    "\n",
    "Overfitting will be addressed thru hyperparameter tuning, such as pruning trees used in random forests / XGBoost, in addition to other parameter tuning. \n",
    "\n",
    "This is a multi-class classification problem, with three available class labels (Neutral, Positive, or Negative). As a result, the performance metric we will focus on throughout this process will be accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate random forest classifiers, with balanced class_weight\n",
    "rf_count = RandomForestClassifier(random_state=SEED, n_jobs=-1, class_weight='balanced')\n",
    "rf_tfidf = RandomForestClassifier(random_state=SEED, n_jobs=-1, class_weight='balanced')\n",
    "rf_w2v = RandomForestClassifier(random_state=SEED, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "# fit to training sets\n",
    "rf_count.fit(X_train_count, y_train)\n",
    "rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "rf_w2v.fit(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorized Train Score: 0.9512341062079281\n",
      "Count Vectorized Test Score: 0.6630776132794975\n",
      "--------\n",
      "TF-IDF Vectorized Train Score: 0.9512341062079281\n",
      "TF-IDF Vectorized Test Score: 0.6585912965455362\n",
      "--------\n",
      "Word2Vec Vectorized Train Score: 0.9545250560957367\n",
      "Word2Vec Vectorized Test Score: 0.6514131897711979\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorized\n",
    "count_train_score = rf_count.score(X_train_count, y_train)\n",
    "count_test_score = rf_count.score(X_test_count, y_test)\n",
    "print(f'Count Vectorized Train Score: {count_train_score}')\n",
    "print(f'Count Vectorized Test Score: {count_test_score}')\n",
    "print('--------')\n",
    "\n",
    "# TF-IDF Vectorized\n",
    "tfidf_train_score = rf_tfidf.score(X_train_tfidf, y_train)\n",
    "tfidf_test_score = rf_tfidf.score(X_test_tfidf, y_test)\n",
    "print(f'TF-IDF Vectorized Train Score: {tfidf_train_score}')\n",
    "print(f'TF-IDF Vectorized Test Score: {tfidf_test_score}')\n",
    "print('--------')\n",
    "\n",
    "# W2V Vectorized\n",
    "w2v_train_score = rf_w2v.score(X_train_w2v, y_train)\n",
    "w2v_test_score = rf_w2v.score(X_test_w2v, y_test)\n",
    "print(f'Word2Vec Vectorized Train Score: {w2v_train_score}')\n",
    "print(f'Word2Vec Vectorized Test Score: {w2v_test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing baseline random model scores for our three vectorized datasets (count, tf-idf, and word2vec using glove), we can see that results are fairly consistent across our vectorization methods. Further, looking at our high training set accuracy score vs. test scores, shows we are likely overfitting slightly to the training data.  \n",
    "\n",
    "Random forests are known for overfitting and we have not yet tuned any hyperparams.  Set up a grid search to optimize params. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Count Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set search parameters\n",
    "rf_params_count = {\n",
    "    'max_depth': [10, 25, 50, 75, 100, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'n_estimators': [25, 50, 75, 100]\n",
    "}\n",
    "\n",
    "# instantiate random forest classifier for randomized search\n",
    "rf_classifier = RandomForestClassifier(random_state=SEED, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "# instantiate random search\n",
    "rf_count_rs = RandomizedSearchCV(estimator=rf_classifier,\n",
    "                                 param_distributions=rf_params_count,\n",
    "                                 return_train_score=True,\n",
    "                                 scoring='accuracy',\n",
    "                                 n_iter=100,\n",
    "                                 verbose=1,\n",
    "                                 cv=3,\n",
    "                                 random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                                    n_jobs=-1, random_state=1),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'max_depth': [10, 25, 50, 75, 100,\n",
       "                                                      None],\n",
       "                                        'min_samples_leaf': [1, 2, 5],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [25, 50, 75, 100]},\n",
       "                   random_state=1, return_train_score=True, scoring='accuracy',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit to count-vectorized data\n",
    "rf_count_rs.fit(X_train_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search Train Accuracy (Count Vect.): 0.729998733324767\n",
      "Random Search Test Accuracy (Count Vect.): 0.5947952362887449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_depth': 100}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print count-vectorized randomized-search results\n",
    "mean_train_score_count = np.mean(rf_count_rs.cv_results_['mean_train_score'])\n",
    "mean_test_score_count = np.mean(rf_count_rs.cv_results_['mean_test_score'])\n",
    "print(f'Random Search Train Accuracy (Count Vect.): {mean_train_score_count}')\n",
    "print(f'Random Search Test Accuracy (Count Vect.): {mean_test_score_count}')\n",
    "\n",
    "# display best params\n",
    "rf_count_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gridsearch with params around these values to see if results can be improved further\n",
    "# need to further address overfitting\n",
    "grid_search_params = {\n",
    "    'min_samples_split': [4, 5],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'max_depth': [25, 50, 75],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "# instantiate classifier\n",
    "rf_classifier = RandomForestClassifier(n_jobs=-1, random_state=SEED, class_weight='balanced', n_estimators=100)\n",
    "\n",
    "# instantiate grid search\n",
    "rf_gs_count = GridSearchCV(estimator=rf_classifier, \n",
    "                           param_grid=grid_search_params, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy', \n",
    "                           return_train_score=True,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                              n_jobs=-1, random_state=1),\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'criterion': ['entropy', 'gini'],\n",
       "                         'max_depth': [25, 50, 75],\n",
       "                         'max_features': ['auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [3, 4],\n",
       "                         'min_samples_split': [4, 5]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit to count vectorized\n",
    "rf_gs_count.fit(X_train_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search Train Accuracy (Count Vect.): 0.659510896388931\n",
      "Random Search Test Accuracy (Count Vect.): 0.5737062579643757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 75,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 4}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print count-vectorized results\n",
    "mean_train_score_count = np.mean(rf_gs_count.cv_results_['mean_train_score'])\n",
    "mean_test_score_count = np.mean(rf_gs_count.cv_results_['mean_test_score'])\n",
    "print(f'Random Search Train Accuracy (Count Vect.): {mean_train_score_count}')\n",
    "print(f'Random Search Test Accuracy (Count Vect.): {mean_test_score_count}')\n",
    "\n",
    "# display best params\n",
    "rf_gs_count.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuned Random Forest (Count Vectorized) Test Accuracy: 0.62673844773441\n"
     ]
    }
   ],
   "source": [
    "# run best count-vect model with these params\n",
    "best_rf_count = RandomForestClassifier(class_weight='balanced', n_jobs=-1, random_state=SEED,\n",
    "                                       bootstrap=True,\n",
    "                                       criterion='gini',\n",
    "                                       max_depth=75,\n",
    "                                       max_features='auto',\n",
    "                                       min_samples_leaf=3,\n",
    "                                       min_samples_split=4)\n",
    "\n",
    "# fit to count vect data\n",
    "best_rf_count.fit(X_train_count, y_train)\n",
    "\n",
    "# print testing score\n",
    "print(f'Best Tuned Random Forest (Count Vectorized) Test Accuracy: {best_rf_count.score(X_test_count, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our tuned params, we now see we are getting a test accuracy of ~59%, with a train accuracy of ~66%.  Overfitting appears to have been addressed through tuning as evidenced by the closeness of training and testing accuracy.  \n",
    "\n",
    "Results are somewhat strong, especially when considering a \"simple\" model guessing on a balanced dataset would be expected to generate an accuracy score of ~33%. \n",
    "\n",
    "Run similar process with TF-IDF Vectorized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - TF-IDF Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set search parameters\n",
    "rf_params_tfidf = {\n",
    "    'max_depth': [10, 25, 50, 75, 100, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'n_estimators': [25, 50, 75, 100]\n",
    "}\n",
    "\n",
    "# instantiate random forest classifier for randomized search\n",
    "rf_classifier = RandomForestClassifier(random_state=SEED, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "# instantiate random search\n",
    "rf_tfidf_rs = RandomizedSearchCV(estimator=rf_classifier,\n",
    "                                 param_distributions=rf_params_tfidf,\n",
    "                                 return_train_score=True,\n",
    "                                 scoring='accuracy',\n",
    "                                 n_iter=100,\n",
    "                                 verbose=1,\n",
    "                                 cv=3,\n",
    "                                 random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                                    n_jobs=-1, random_state=1),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'max_depth': [10, 25, 50, 75, 100,\n",
       "                                                      None],\n",
       "                                        'min_samples_leaf': [1, 2, 5],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [25, 50, 75, 100]},\n",
       "                   random_state=1, return_train_score=True, scoring='accuracy',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit to vectorized dataset\n",
    "rf_tfidf_rs.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Train Accuracy (TF-IDF): 0.7468722628099855\n",
      "Grid Search Test Accuracy (TF-IDF): 0.5943212311516302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_depth': 100}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print search results\n",
    "mean_train_score_tfidf = np.mean(rf_tfidf_rs.cv_results_['mean_train_score']) \n",
    "mean_test_score_tfidf = np.mean(rf_tfidf_rs.cv_results_['mean_test_score'])\n",
    "print(f'Grid Search Train Accuracy (TF-IDF): {mean_train_score_tfidf}')\n",
    "print(f'Grid Search Test Accuracy (TF-IDF): {mean_test_score_tfidf}')\n",
    "\n",
    "# display best params\n",
    "rf_tfidf_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to count vectorized, run refined grid search and try to further address overfitting\n",
    "grid_search_params = {\n",
    "    'min_samples_split': [4, 5],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'max_depth': [25, 50, 75],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "# instantiate classifier\n",
    "rf_classifier = RandomForestClassifier(n_jobs=-1, random_state=SEED, class_weight='balanced', n_estimators=100)\n",
    "\n",
    "# instantiate grid search\n",
    "rf_gs_tfidf = GridSearchCV(estimator=rf_classifier, \n",
    "                           param_grid=grid_search_params, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy', \n",
    "                           return_train_score=True,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                              n_jobs=-1, random_state=1),\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'criterion': ['entropy', 'gini'],\n",
       "                         'max_depth': [25, 50, 75],\n",
       "                         'max_features': ['auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [3, 4],\n",
       "                         'min_samples_split': [4, 5]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit to training data\n",
    "rf_gs_tfidf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Train Accuracy (TF-IDF): 0.6863403538168945\n",
      "Grid Search Test Accuracy (TF-IDF): 0.5787108687891517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 75,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 4}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print search results\n",
    "mean_train_score_tfidf = np.mean(rf_gs_tfidf.cv_results_['mean_train_score']) \n",
    "mean_test_score_tfidf = np.mean(rf_gs_tfidf.cv_results_['mean_test_score'])\n",
    "print(f'Grid Search Train Accuracy (TF-IDF): {mean_train_score_tfidf}')\n",
    "print(f'Grid Search Test Accuracy (TF-IDF): {mean_test_score_tfidf}')\n",
    "\n",
    "# display best params\n",
    "rf_gs_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Tuned Random Forest (Count Vectorized) Test Accuracy: 0.6258411843876177\n"
     ]
    }
   ],
   "source": [
    "# run best tfidf model with identified params\n",
    "best_rf_tfidf = RandomForestClassifier(random_state=SEED, n_jobs=-1, class_weight='balanced', n_estimators=100,\n",
    "                                       min_samples_split=4, \n",
    "                                       min_samples_leaf=3,\n",
    "                                       max_depth=75,\n",
    "                                       max_features='auto',\n",
    "                                       bootstrap=True,\n",
    "                                       criterion='gini')\n",
    "\n",
    "# fit to tfidf vectorized\n",
    "best_rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# print testing score\n",
    "print(f'Best Tuned Random Forest (Count Vectorized) Test Accuracy: {best_rf_tfidf.score(X_test_tfidf, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of TF-IDF vectorized data is similar to that of count vectorized.  With testing scores ~59% and training scores close to 69%.  Results may be overfitting slightly more to the TF-IDF vectorized data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Word2Vec Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set grid search parameters\n",
    "rf_params_w2v = {\n",
    "    'max_depth': [10, 25, 50, 75, 100, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'n_estimators': [25, 50, 75, 100]\n",
    "}\n",
    "\n",
    "# instantiate random forest classifier for grid search\n",
    "rf_classifier = RandomForestClassifier(random_state=SEED, class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "# instantiate grid search\n",
    "rf_w2v_rs = RandomizedSearchCV(estimator=rf_classifier,\n",
    "                               param_distributions=rf_params_w2v,\n",
    "                               return_train_score=True,\n",
    "                               scoring='accuracy',\n",
    "                               n_iter=100,\n",
    "                               verbose=1,\n",
    "                               cv=3,\n",
    "                               random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                                    n_jobs=-1, random_state=1),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'max_depth': [10, 25, 50, 75, 100,\n",
       "                                                      None],\n",
       "                                        'min_samples_leaf': [1, 2, 5],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [25, 50, 75, 100]},\n",
       "                   random_state=1, return_train_score=True, scoring='accuracy',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train to dataset\n",
    "rf_w2v_rs.fit(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Train Accuracy (word2vec): 0.9582281511194074\n",
      "Grid Search Test Accuracy (word2vec): 0.6426536295000428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 75,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_depth': None}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print word2vec vectorized grid search results\n",
    "mean_train_score_w2v = np.mean(rf_w2v_rs.cv_results_['mean_train_score']) \n",
    "mean_test_score_w2v = np.mean(rf_w2v_rs.cv_results_['mean_test_score'])\n",
    "print(f'Grid Search Train Accuracy (word2vec): {mean_train_score_w2v}')\n",
    "print(f'Grid Search Test Accuracy (word2vec): {mean_test_score_w2v}')\n",
    "\n",
    "# display best params\n",
    "rf_w2v_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further refine with grid search and further address overfitting\n",
    "rf_params_w2v = {\n",
    "    'min_samples_split': [5, 6],\n",
    "    'min_samples_leaf': [3, 4],\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['entropy', 'gini']\n",
    "}\n",
    "\n",
    "# instantiate classifier\n",
    "rf_classifier = RandomForestClassifier(n_jobs=-1, random_state=SEED, class_weight='balanced', n_estimators=75)\n",
    "\n",
    "# instantiate grid search\n",
    "rf_gs_w2v = GridSearchCV(estimator=rf_classifier, \n",
    "                           param_grid=rf_params_w2v, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy', \n",
    "                           return_train_score=True,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                              n_estimators=75, n_jobs=-1,\n",
       "                                              random_state=1),\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'criterion': ['entropy', 'gini'],\n",
       "                         'max_depth': [5, 6, 7],\n",
       "                         'max_features': ['auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [3, 4],\n",
       "                         'min_samples_split': [5, 6]},\n",
       "             return_train_score=True, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_gs_w2v.fit(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Train Accuracy (word2vec): 0.6980460028180712\n",
      "Grid Search Test Accuracy (word2vec): 0.5374097277540485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 7,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 5}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print word2vec vectorized grid search results\n",
    "mean_train_score_w2v = np.mean(rf_gs_w2v.cv_results_['mean_train_score']) \n",
    "mean_test_score_w2v = np.mean(rf_gs_w2v.cv_results_['mean_test_score'])\n",
    "print(f'Grid Search Train Accuracy (word2vec): {mean_train_score_w2v}')\n",
    "print(f'Grid Search Test Accuracy (word2vec): {mean_test_score_w2v}')\n",
    "\n",
    "# display best params\n",
    "rf_gs_w2v.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a different range for w2v vect data as overfitting appeared stronger. Now that overfitting has been addressed and a number of hyperparams have been set, we can see performance is still not great.  Move on to SVC to see if we can improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(class_weight='balanced', max_iter=5000, random_state=1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create linear SVC\n",
    "svc_count = LinearSVC(random_state=SEED, class_weight='balanced', max_iter=5000)\n",
    "svc_tfidf = LinearSVC(random_state=SEED, class_weight='balanced', max_iter=5000)\n",
    "svc_w2v = LinearSVC(random_state=SEED, class_weight='balanced', max_iter=5000)\n",
    "\n",
    "# fit to training sets\n",
    "svc_count.fit(X_train_count, y_train)\n",
    "svc_tfidf.fit(X_train_tfidf, y_train)\n",
    "svc_w2v.fit(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorized Train Score: 0.8764397905759163\n",
      "Count Vectorized Test Score: 0.6464782413638402\n",
      "TF-IDF Vectorized Train Score: 0.8480179506357517\n",
      "TF-IDF Vectorized Test Score: 0.6401973979362943\n",
      "Word2Vect Vectorized Train Score: 0.5992520568436799\n",
      "Word2Vect Vectorized Test Score: 0.5895020188425303\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorized\n",
    "count_train_score = svc_count.score(X_train_count, y_train)\n",
    "count_test_score = svc_count.score(X_test_count, y_test)\n",
    "print(f'Count Vectorized Train Score: {count_train_score}')\n",
    "print(f'Count Vectorized Test Score: {count_test_score}')\n",
    "\n",
    "# TF-IDF Vectorized\n",
    "tfidf_train_score = svc_tfidf.score(X_train_tfidf, y_train)\n",
    "tfidf_test_score = svc_tfidf.score(X_test_tfidf, y_test)\n",
    "print(f'TF-IDF Vectorized Train Score: {tfidf_train_score}')\n",
    "print(f'TF-IDF Vectorized Test Score: {tfidf_test_score}')\n",
    "\n",
    "# Word2Vec Vectorized\n",
    "w2v_train_score = svc_w2v.score(X_train_w2v, y_train)\n",
    "w2v_test_score = svc_w2v.score(X_test_w2v, y_test)\n",
    "print(f'Word2Vect Vectorized Train Score: {w2v_train_score}')\n",
    "print(f'Word2Vect Vectorized Test Score: {w2v_test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at baseline linearSVC results, we can see that we are not overfitting as badly to training data as we were with random forest model above. Baseline test scores are inline with each other when comparing Count and TF-IDF vectorized datasets. \n",
    "\n",
    "Try to addres overfitting and improve results with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params for grid search\n",
    "svc_params = {\n",
    "    'C': [0.01, 0.1, 1, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/addingtongraham/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/addingtongraham/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/addingtongraham/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/addingtongraham/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/addingtongraham/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(class_weight='balanced', max_iter=10000,\n",
       "                                 random_state=1),\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10]}, return_train_score=True,\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search \n",
    "svc_classifier = LinearSVC(random_state=SEED, class_weight='balanced', max_iter=10000)\n",
    "svc_grid_search = GridSearchCV(svc_classifier,\n",
    "                               svc_params,\n",
    "                               return_train_score=True,\n",
    "                               scoring='accuracy')\n",
    "\n",
    "# fit to count data\n",
    "svc_grid_search.fit(X_train_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Train Accuracy (Count Vect.): 0.8609573672400898\n",
      "Grid Search Test Accuracy (Count Vect.): 0.6398279730740464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print count-vectorized grid-search results\n",
    "mean_train_score_count = np.mean(svc_grid_search.cv_results_['mean_train_score'])\n",
    "mean_test_score_count = np.mean(svc_grid_search.cv_results_['mean_test_score'])\n",
    "print(f'Grid Search Train Accuracy (Count Vect.): {mean_train_score_count}')\n",
    "print(f'Grid Search Test Accuracy (Count Vect.): {mean_test_score_count}')\n",
    "\n",
    "# display best params\n",
    "svc_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(class_weight='balanced', max_iter=10000,\n",
       "                                 random_state=1),\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10]}, return_train_score=True,\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# failing to converge, try with tfidf vectorized data\n",
    "svc_grid_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Train Accuracy (TF-IDF): 0.8109667165295438\n",
      "Grid Search Test Accuracy (TF-IDF): 0.6428571428571428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print tfidf-vectorized grid search results\n",
    "mean_train_score_tfidf = np.mean(svc_grid_search.cv_results_['mean_train_score']) \n",
    "mean_test_score_tfidf = np.mean(svc_grid_search.cv_results_['mean_test_score'])\n",
    "print(f'Grid Search Train Accuracy (TF-IDF): {mean_train_score_tfidf}')\n",
    "print(f'Grid Search Test Accuracy (TF-IDF): {mean_test_score_tfidf}')\n",
    "\n",
    "# display best params\n",
    "svc_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LinearSVC, our TF-IDF vectorized data appears to be performing slightly better, with slighty less overfitting to training data.  Additionally, test score results are marginally better than the \"dummy\" model that guess neutral every time. Moving forward, we will stick with TF-IDF Vectorization and try to remvoe some more overfitting that is occurring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(class_weight='balanced', max_iter=10000,\n",
       "                                 random_state=1),\n",
       "             param_grid={'C': [0.001, 0.01]}, return_train_score=True,\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_params = {\n",
    "    'C': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# grid search \n",
    "svc_classifier = LinearSVC(random_state=SEED, class_weight='balanced', max_iter=10000)\n",
    "svc_grid_search = GridSearchCV(svc_classifier,\n",
    "                               svc_params,\n",
    "                               return_train_score=True,\n",
    "                               scoring='accuracy')\n",
    "\n",
    "# fit to tfidf data\n",
    "svc_grid_search.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Train Accuracy (TF-IDF): 0.6357329842931937\n",
      "Grid Search Test Accuracy (TF-IDF): 0.6218399401645475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.01}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print tfidf-vectorized grid search results\n",
    "mean_train_score_tfidf = np.mean(svc_grid_search.cv_results_['mean_train_score']) \n",
    "mean_test_score_tfidf = np.mean(svc_grid_search.cv_results_['mean_test_score'])\n",
    "print(f'Grid Search Train Accuracy (TF-IDF): {mean_train_score_tfidf}')\n",
    "print(f'Grid Search Test Accuracy (TF-IDF): {mean_test_score_tfidf}')\n",
    "\n",
    "# display best params\n",
    "svc_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running updated grid search on tf-idf vectorized data, we see some improved results. Looking at our training accuracy score of 63.6%, we are inline with our test accuracy score of 62%.  We are no longer likely overfitting to training data.  While results are not particularly strong, we are still outperforming the \"dummy\" model, which would score closer to 60%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(class_weight='balanced', max_iter=10000,\n",
       "                                 random_state=1),\n",
       "             param_grid={'C': [0.001, 0.01]}, return_train_score=True,\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit to w2v vectorized data\n",
    "svc_grid_search.fit(X_train_w2v, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Train Accuracy (Word2Vec): 0.6006731488406881\n",
      "Grid Search Test Accuracy (Word2Vec): 0.5915482423335827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.001}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print w2v-vectorized grid search results\n",
    "mean_train_score_w2v = np.mean(svc_grid_search.cv_results_['mean_train_score']) \n",
    "mean_test_score_w2v = np.mean(svc_grid_search.cv_results_['mean_test_score'])\n",
    "print(f'Grid Search Train Accuracy (Word2Vec): {mean_train_score_w2v}')\n",
    "print(f'Grid Search Test Accuracy (Word2Vec): {mean_test_score_w2v}')\n",
    "\n",
    "# display best params\n",
    "svc_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.01, class_weight='balanced', max_iter=10000, random_state=1)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a best version of the SVC model\n",
    "best_svc = LinearSVC(class_weight='balanced',\n",
    "                     max_iter=10000,\n",
    "                     random_state=SEED,\n",
    "                     C=0.01)\n",
    "\n",
    "# fit to data\n",
    "best_svc.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Identified SVC Train Accuracy: 0.6785340314136126\n",
      "Best Identified SVC Test Accuracy: 0.6541049798115747\n"
     ]
    }
   ],
   "source": [
    "# generate scores\n",
    "print(f'Best Identified SVC Train Accuracy: {best_svc.score(X_train_tfidf, y_train)}')\n",
    "print(f'Best Identified SVC Test Accuracy: {best_svc.score(X_test_tfidf, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to one-hot encoded format\n",
    "y_train_encoded = pd.get_dummies(y_train).values\n",
    "y_test_encoded = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer and limit overall vocab size to 20000 most important words\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "\n",
    "# fit on text\n",
    "tokenizer.fit_on_texts(list(X_train_preprocessed))\n",
    "list_tokenized = tokenizer.texts_to_sequences(X_train_preprocessed)\n",
    "X_t = sequence.pad_sequences(list_tokenized, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set embedding size\n",
    "embedding_size = 50\n",
    "\n",
    "# construct neural network\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, embedding_size))\n",
    "model.add(LSTM(25, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5)) # to help overfitting\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5)) # to help overfitting\n",
    "model.add(Dense(3, activation='softmax')) # 3 potential class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model with params\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 25)          7600      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 25)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 1,009,053\n",
      "Trainable params: 1,009,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/addingtongraham/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6016 samples, validate on 669 samples\n",
      "Epoch 1/5\n",
      "2350/6016 [==========>...................] - ETA: 17s - loss: 0.9865 - accuracy: 0.4838"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-f2cd764c93f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/keras-env/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_t, y_train_encoded, epochs=5, batch_size=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
